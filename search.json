[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lab Assignments",
    "section": "",
    "text": "Site overview\nThis site contains the lab assignments and their solutions (following the lab class).\n\n\nLab 1\nIn this lab we will cover the following topics:\n\nImporting datasets into RStudio;\nTesting normality of data;\nCreating conventional and bootstrap confidence intervals.\nHere are the solutions: Lab 1 - Solutions\n\n\n\nLab 2\nIn this lab we will cover the following topics:\n\nVisualising data;\nThe Mann-Whitney U test;\nThe Wilcoxon signed-rank test.\nHere are the solutions: Lab 2 - Solutions\n\n\n\nLab 3\nIn this lab we will cover the following topics:\n\nThe Kruskal-Wallis H test;\nDunn post hoc testing;\nThe Friedman test;\nNemenyi post hoc testing.\nHere are the solutions: Lab 3 - Solutions\n\n\n\nLab 4\nIn this lab we will cover the following topics:\n\nThe chi-square distribution;\nThe chi-square goodness-of-fit test.\nHere are the solutions: Lab 4 - Solutions\n\n\n\nLab 5\nIn this lab we will cover the following topics:\n\nThe chi-square test of association between categorical variables;\nThe linear-by-linear option when testing for association between ordinal variables.\nHere are the solutions: Lab 5 - Solutions\n\n\n\nLab 6\nIn this lab we will cover the following topics:\n\nThe hypergeometric distribution;\nFisher’s Exact Test.\nHere are the solutions: Lab 6 - Solutions\n\n\n\nLab 7\nIn this lab we will cover the following topics:\n\nThe Poisson Distribution;\nPoisson regression/log-linear models.\nHere are the solutions: Lab 7 - Solutions\n\n\n\nLab 8\nIn this lab we will cover the following topics:\n\nThe Binomial Distribution;\nLogistic regression.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "MA-292 lab 2 site.html",
    "href": "MA-292 lab 2 site.html",
    "title": "Lab 2",
    "section": "",
    "text": "Please note that there is a file on Canvas called Getting started with R which may be of some use. This provides details of setting up R and Rstudio on your own computer as well as providing an overview of inputting and importing various data files into R. This should mainly serve as a reminder.\nRecall that we can clear the environment using rm(list=ls()) It is advisable to do this before attempting new questions if confusion may arise with variable names etc.\n\nExample 1\nIn this example we will elaborate on one of the lecture examples where we investigated the effectiveness of two different asthma drugs. The data is provided in the Asthma Drugs dataset, and the readings are peak flow meter differences for the two drugs. The dataset must first be adapted into an R friendly form, i.e. create a new column with all the peak flow measurements in one column, which I have called PFD, with an associated grouping variable, which I have called Drug. Remember to save your file as a csv file. Below is a snippet of my revised dataset:\n\nNext, follow the step-by-step guide below,\n\nFirstly, we input the dataset into R, check it using the “head” command (recall this provides the first 6 entries) and attach it.\n\nAsthma&lt;-read.csv(file.choose(), header=T)\nor\n\nAsthma&lt;-read.csv(\"Asthma Drugs Revised.csv\", header=T)\nhead(Asthma)\n\n   A  B  X X.1 PFD Drug\n1  5 30 NA  NA   5    1\n2  7 25 NA  NA   7    1\n3 20  2 NA  NA  20    1\n4  9 30 NA  NA   9    1\n5 25 35 NA  NA  25    1\n6  2 40 NA  NA   2    1\n\nattach(Asthma)\n\n\nIf the two samples are normally distributed then this would be the situation of an independent samples t-test (seen in MA-192). Therefore, we will first check the normality of drugs A and B. To do this, use the following code\n\n\nshapiro.test(A)\n\n\n    Shapiro-Wilk normality test\n\ndata:  A\nW = 0.87786, p-value = 0.1233\n\nshapiro.test(B)\n\n\n    Shapiro-Wilk normality test\n\ndata:  B\nW = 0.82685, p-value = 0.01923\n\n\nand\ninstall.packages(\"nortest\")\n\nlibrary(nortest)\nlillie.test(A)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  A\nD = 0.24576, p-value = 0.08832\n\nlillie.test(B)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  B\nD = 0.24234, p-value = 0.04996\n\n\n\nClearly the data are not normally distributed, therefore we proceed with the non-parametric equivalent test, the Mann-Whitney U-test. Before doing this, we investigate the shape of the distributions of the data. We will do this by creating histograms of the data. We create a factor for Drug and create side-by-side histograms for an easier comparison, noting the choice of “breaks” to provide better presentation, see the code below:\n\n\nDrug&lt;-factor(Drug,c(1,2),labels=c('Drug A','Drug B'))\npar(mfrow=c(1,2)) \nhist(PFD[Drug=='Drug A'], xlab='PFD', main='Histogram for Drug A', breaks=seq(from=0,to=40,by=5)) \nhist(PFD[Drug=='Drug B'], xlab='PFD', main='Histogram for Drug B', breaks=seq(from=0,to=40,by=5))\n\n\n\n\n\nThe histograms produced should show that both drugs have similar shaped distributions. Recall from the lectures that this allows us to effectively compare medians using the Mann-Whitney U-test. Without this, we would be testing whether the two groups follow the same distribution.\nTo perform the Mann-Whitney U-test, use the code below (Please note that the default option for pairing is false, i.e. a Mann-Whitney U test and not a Wilcoxon Signed-Rank test):\n\nwilcox.test(PFD~Drug)\n\nYou will have obtain an error concerning ties when you use this approach. For cases where we have ties in the data we have to use the option ``exact=F”, see below:\n\n\nwilcox.test(PFD~Drug, exact=F)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  PFD by Drug\nW = 39, p-value = 0.1739\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nNote that we may use the original data, i.e. side-by-side columns, but we have to adapt the code slightly to:\n\n\nwilcox.test(A,B, exact=F)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  A and B\nW = 39, p-value = 0.1739\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nIn both approaches check that you obtain a p-value of \\(0.1739&gt;0.05\\), therefore we do not reject \\(H_0\\) and conclude that there is no difference between the medians of the two asthma drugs.\nPlease note that there is also the wilcox_test command within the “coin” package that can deal with ties, see\n\ninstall.packages(\"coin\")\n\nlibrary(coin)\n\nLoading required package: survival\n\nwilcox_test(PFD~Drug)\n\n\n    Asymptotic Wilcoxon-Mann-Whitney Test\n\ndata:  PFD by Drug (Drug A, Drug B)\nZ = -1.393, p-value = 0.1636\nalternative hypothesis: true mu is not equal to 0\n\n\n\nWe obtain the same conclusion with this technique.\n\n\n\nExercise 1\nThe Reaction Times dataset contains the results of a trial measuring reaction times of 2 different groups of people, where one group is given water before the trial and the other group is given a new drink designed to reduce reaction times. Perform an appropriate statistical test to determine whether there is any difference in reaction times. Calculate the effect size.\nHint: Note that in order to obtain a \\(Z\\) value to calculate the effect size, the wilcox_test() command from the “coin” package will need to be used. Also remember how to import an SPSS file: either using the foreign package or use the Import Dataset command under Environment.\n\n\nExample 2\nIn this example we will perform Example 2.9 in R. Using the Public Awareness dataset, we investigate whether there is any difference between a certain video (Video A) and a method using props (Demo D) in informing the public about a certain medical condition. The same participants evaluate each method. Please follow the steps below:\n\nFirstly use the “Import Dataset” command in the “Environment” to import the dataset.\nWe could be mistaken to think that this is a situation where we use a paired t-test. Perform a test of normality on the differences to check one of the main conditions of this test using,\n\n\nshapiro.test(Public_Awareness$Difference)\n\n\n    Shapiro-Wilk normality test\n\ndata:  Public_Awareness$Difference\nW = 0.95299, p-value = 0.4147\n\n\ninstall.packages(\"nortest\")\n\nlibrary(nortest)\nlillie.test(Public_Awareness$Difference)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  Public_Awareness$Difference\nD = 0.15672, p-value = 0.2223\n\n\n\nNote that we do not have evidence to reject the normality of the data. However, if we look carefully at the dataset, we see that we are dealing with ordinal data and therefore the Wilcoxon Signed-Rank test is an option (the assumption of continuous data of the paired t-test is not met since there are only 4 ordered categories - recall that 7 or more are required to be classed as continuous).\nWe next investigate the shape of the differences using a histogram and boxplot in order to determine the correct hypotheses, see\n\n\nhist(Public_Awareness$Difference, xlab='Difference', main='Histogram for Differences', breaks=seq(from=-5,to=10, by=2))\n\n\n\nboxplot(Public_Awareness$Difference, main=\"Box Plot of Differences\", ylab=\"Differences\", xlab=\"Video A - Demo D\", las=1)\n\n\n\n\n\nFrom your output you should be able to see that the differences do appear to be symmetric.\nNow we proceed to the Wilcoxon Signed-Rank test.\n\n\nwilcox.test(Public_Awareness$TotalAGen, Public_Awareness$TotalDDEMO, paired=TRUE, exact=F)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  Public_Awareness$TotalAGen and Public_Awareness$TotalDDEMO\nV = 167.5, p-value = 0.003558\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nThe test statistic is significant, therefore we conclude that median difference is significantly different to 0.\nThe medians can be calculated using\n\n\nmedian(Public_Awareness$TotalAGen)\n\n[1] 24\n\nmedian(Public_Awareness$TotalDDEMO)\n\n[1] 22\n\n\n\nAs the median of Video A is 24 and that of Demo D is 22, we conclude that Video A seems to be more effective.\nThe coin package has an alternative function for this procedure, however the data must be presented differently, i.e. one column containing the data and a grouping variable. I have created a new file called Public Awareness Revised to do this. Import this dataset and use the relevant code below:\n\n\nwilcoxsign_test(Public_Awareness_Revised$Effectiveness~Public_Awareness_Revised$Method)\n\n\n    Asymptotic Wilcoxon-Pratt Signed-Rank Test\n\ndata:  y by x (pos, neg) \n     stratified by block\nZ = 5.5779, p-value = 2.434e-08\nalternative hypothesis: true mu is not equal to 0\n\n\n\n\nExercise 2\nThe Reorganisation Salaries dataset contains the salaries of individual employees before and after a reorganisation. Investigate whether there is any statistically significant difference in the salaries before and after the reorganisation, justifying fully the procedure used.\nHint: The differences are not provided in the dataset, therefore you must create a new dataset which includes a new column containing the differences. Either do this in Excel before importing the data or use the following code in R:\nSalaries.new &lt;- transform(Salaries, Differences = Before - After)\nSalaries.new\n\n\n\n\n Back to top"
  },
  {
    "objectID": "MA-292 lab 1 site.html",
    "href": "MA-292 lab 1 site.html",
    "title": "Lab 1",
    "section": "",
    "text": "Please note that there is a file on Canvas called Getting started with R which may be of some use. This provides details of setting up R and Rstudio on your own computer as well as providing an overview of inputting and importing various data files into R. This should mainly serve as a reminder.\nRecall that we can clear the environment using rm(list=ls()) It is advisable to do this before attempting new questions if confusion may arise with variable names etc.\n\nExample 1\nThis example will provide the full details of the confidence interval example we met in the lectures. We will create a confidence interval for the variable StudentAge in the Student Age dataset. Please follow the steps below:\n\nFirstly, we input the dataset into R, attach it and check it using the “head” command (recall that this provides the first 6 entries).\n\n\nlibrary(foreign)\nStudent &lt;- read.spss(\"Student Age.sav\", to.data.frame=T)\n\nOr, if you prefer to search for the dataset file, use:\nStudent &lt;- read.spss(file.choose(), to.data.frame=T)\n\nattach(Student)\nhead(Student)\n\n  StudentAge\n1         55\n2         60\n3         58\n4         20\n5         22\n6         59\n\n\n\nIn order to determine the correct method, we test the normality of the data. We first do this visually - we first consider a basic histogram:\n\n\nhist(StudentAge)\n\n\n\n\n\nThe code below produces a histogram with an overlaid probability density function. Make sure you understand the code and the output generated.\n\n\nhist(StudentAge, prob=T, breaks=seq(from=0,to=100, by=10), xlab=\"Student Age\", \n     main=\"Histogram of Student Age\", las=1)\nlines(density(StudentAge),col=2)\n\n\n\n\n\nNext we perform the Shapiro-Wilk test and the Kolmogorov-Smirnov test (with Lilliefors’ correction). Note that we require the “nortest” package to use Lilliefors’ test.\n\n\nshapiro.test(StudentAge)\n\n\n    Shapiro-Wilk normality test\n\ndata:  StudentAge\nW = 0.83438, p-value = 0.02367\n\n\ninstall.packages(\"nortest\")\n\nlibrary(nortest)\nlillie.test(StudentAge)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  StudentAge\nD = 0.25006, p-value = 0.03682\n\n\n\nYou should have come to the conclusion the the variable StudentAge is not normally distributed. Therefore, we cannot use traditional parametric techniques to produce confidence intervals (as seen in MA-192) and hence we use the bootstrap method. The code below is used to calculate the mean of each bootstrap sample and m.mean is just the name of the function to calculate the mean of each bootstrap sample with index i. We use the standard 1000 resamples. Note that we must use the boot package.\n\n\nlibrary(boot)\nmy.mean &lt;- function(x, i){return( mean( x[i] ) )}\nage.boot &lt;- boot(Student$StudentAge, my.mean, 1000)\n\n\nThe code below is then used to produce bootstrap confidence intervals. Note the options, e.g. changing the significance level and specifying the type of bootstrap C.I. All three types of bootstrap confidence interval seen in the lectures are included below.\n\n\nboot.ci(age.boot, conf=0.95, type=c(\"perc\", \"bca\", \"norm\"))\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = age.boot, conf = 0.95, type = c(\"perc\", \"bca\", \n    \"norm\"))\n\nIntervals : \nLevel      Normal             Percentile            BCa          \n95%   (36.09, 57.01 )   (36.42, 56.83 )   (35.48, 56.28 )  \nCalculations and Intervals on Original Scale\n\n\n\nYou should obtain an output similar to the one above in your Console, noting that bootstrapping is a random process so the actual intervals will vary.\n\n\n\n\nExercise 1\nCreate a 95% confidence interval for the BMI variable in the BMI Scores dataset. Firstly, test the normality of the data, and if a bootstrap confidence interval is required create both percentile and BCa bootstrap confidence intervals for the mean of BMI.\nHint: The dataset needs to be made R friendly and you need to think about how to import it into R. One way is to save it as a csv file and use the code: BMIScores&lt;-read.csv(file.choose(), header=T)\n\n\nExercise 2\nWhy would we not perform a bootstrap confidence interval on the variable Heightincm in the dataset MA-777 Student Height? Give the standard confidence interval for the mean of student height (use the code t.test(\"variable name\", conf.level=0.95), as used in MA-192).\nHint: Another way to import Excel files into R is using the Import Dataset option.\n\n\nExample 2\nIn this example we will investigate skewness of the variable Salary in the Salary (Skewness) dataset. Follow the steps below:\n\nFirstly, import the SPSS dataset into R and check the first 6 entries:\n\nSalaryData &lt;- read.spss(file.choose(), to.data.frame=T)\nor\n\nSalaryData &lt;- read.spss(\"Salary (Skewness).sav\", to.data.frame=T)\nattach(SalaryData)\nhead(SalaryData)\n\n  Salary\n1  22900\n2  22500\n3  23000\n4  19500\n5  19000\n6  15200\n\n\n\nOne option to investigate skewness is to produce a histogram, but we should first take a look at the data to get a better idea of the range of values and what “breaks” we should use, see below:\n\n\nSalaryData\n\n   Salary\n1   22900\n2   22500\n3   23000\n4   19500\n5   19000\n6   15200\n7   17000\n8   19900\n9   45000\n10  60000\n11  18100\n12  10300\n13  22250\n14  22500\n15  30200\n16  33000\n17  37000\n18  20000\n19  21500\n20  50000\n21  31200\n22  27500\n23  25300\n24  26300\n25  28250\n26   5000\n27   6200\n28  36700\n29  40200\n30  23400\n31  18700\n32  19400\n33  24000\n34  72000\n35  85000\n36   5500\n37   7000\n38  15000\n39  37500\n40  92000\n41  36000\n42  42000\n43  51100\n\nhist(Salary, prob=F, breaks=seq(from=2000,to=100000, by=9000),\n     xlab=\"Salary\", main=\"Histogram of Salary Data\", las=1)\n\n\n\n\n\nYou should obtain the above output, indicating left-skewed data.\n\n\n\nNext we calculate the skewness statistic, for which we require the moments package.\n\ninstall.packages(\"moments\")\n\nlibrary(moments)\nskewness(Salary)\n\n[1] 1.515277\n\n\n\nYou should obtain the value 1.515 (to 3dp), as per the output above, providing further evidence of skewness (since the skewness statistic is \\(&gt;1\\)).\n\n\n\nExercise 3\nThe Student Current Account Balance dataset contains the current account balance of 18 students at the end of term. Check the skewness of the variable Balance. Why is a bootstrap confidence interval appropriate in this case? Find a 95% BCa bootstrap confidence interval for the mean of Balance.\nHint: note that the sample size is small. Skewness of data suggests it is not normally distributed, but this can be double-checked using the usual tests.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "MA-292 lab 1 site - with solutions.html",
    "href": "MA-292 lab 1 site - with solutions.html",
    "title": "Lab 1 - with solutions",
    "section": "",
    "text": "Please note that there is a file on Canvas called Getting started with R which may be of some use. This provides details of setting up R and Rstudio on your own computer as well as providing an overview of inputting and importing various data files into R. This should mainly serve as a reminder.\nRecall that we can clear the environment using rm(list=ls()) It is advisable to do this before attempting new questions if confusion may arise with variable names etc.\n\nExample 1\nThis example will provide the full details of the confidence interval example we met in the lectures. We will create a confidence interval for the variable StudentAge in the Student Age dataset. Please follow the steps below:\n\nFirstly, we input the dataset into R, attach it and check it using the “head” command (recall that this provides the first 6 entries).\n\n\nlibrary(foreign)\nStudent &lt;- read.spss(\"Student Age.sav\", to.data.frame=T)\n\nOr, if you prefer to search for the dataset file, use:\nStudent &lt;- read.spss(file.choose(), to.data.frame=T)\n\nattach(Student)\nhead(Student)\n\n  StudentAge\n1         55\n2         60\n3         58\n4         20\n5         22\n6         59\n\n\n\nIn order to determine the correct method, we test the normality of the data. We first do this visually - we first consider a basic histogram:\n\n\nhist(StudentAge)\n\n\n\n\n\n\n\n\n\nThe code below produces a histogram with an overlaid probability density function. Make sure you understand the code and the output generated.\n\n\nhist(StudentAge, prob=T, breaks=seq(from=0,to=100, by=10), xlab=\"Student Age\", \n     main=\"Histogram of Student Age\", las=1)\nlines(density(StudentAge),col=2)\n\n\n\n\n\n\n\n\n\nNext we perform the Shapiro-Wilk test and the Kolmogorov-Smirnov test (with Lilliefors’ correction). Note that we require the “nortest” package to use Lilliefors’ test.\n\n\nshapiro.test(StudentAge)\n\n\n    Shapiro-Wilk normality test\n\ndata:  StudentAge\nW = 0.83438, p-value = 0.02367\n\n\ninstall.packages(\"nortest\")\n\nlibrary(nortest)\nlillie.test(StudentAge)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  StudentAge\nD = 0.25006, p-value = 0.03682\n\n\n\nYou should have come to the conclusion the the variable StudentAge is not normally distributed. Therefore, we cannot use traditional parametric techniques to produce confidence intervals (as seen in MA-192) and hence we use the bootstrap method. The code below is used to calculate the mean of each bootstrap sample and m.mean is just the name of the function to calculate the mean of each bootstrap sample with index i. We use the standard 1000 resamples. Note that we must use the boot package.\n\n\nlibrary(boot)\nmy.mean &lt;- function(x, i){return( mean( x[i] ) )}\nage.boot &lt;- boot(Student$StudentAge, my.mean, 1000)\n\n\nThe code below is then used to produce bootstrap confidence intervals. Note the options, e.g. changing the significance level and specifying the type of bootstrap C.I. All three types of bootstrap confidence interval seen in the lectures are included below.\n\n\nboot.ci(age.boot, conf=0.95, type=c(\"perc\", \"bca\", \"norm\"))\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = age.boot, conf = 0.95, type = c(\"perc\", \"bca\", \n    \"norm\"))\n\nIntervals : \nLevel      Normal             Percentile            BCa          \n95%   (35.10, 57.66 )   (34.92, 57.92 )   (33.87, 57.13 )  \nCalculations and Intervals on Original Scale\n\n\n\nYou should obtain an output similar to the one above in your Console, noting that bootstrapping is a random process so the actual intervals will vary.\n\n\n\n\nExercise 1\nCreate a 95% confidence interval for the BMI variable in the BMI Scores dataset. Firstly, test the normality of the data, and if a bootstrap confidence interval is required create both percentile and BCa bootstrap confidence intervals for the mean of BMI.\nHint: The dataset needs to be made R friendly and you need to think about how to import it into R. One way is to save it as a csv file and use the code: BMIScores&lt;-read.csv(file.choose(), header=T)\n\n\nSolution\n\nNote that this dataset will need to be “tidied” first.\n\nBMIScores &lt;- read.csv(file.choose(), header=T)\n\nhead(BMIScores)\n\n  Patient BMI\n1       1  32\n2       2  29\n3       3  33\n4       4  15\n5       5  16\n6       6  16\n\nattach(BMIScores)\n\n\nWe now visualise the data.\n\n\nhist(BMI, prob=T, breaks=seq(from=0,to=100, by=10), xlab=\"BMI\", main=\"Histogram of BMI Scores\", las=1)\nlines(density(BMI),col=2)\n\n\n\n\n\n\n\n\n\nNext we perform tests of normality.\n\n\nshapiro.test(BMI)\n\n\n    Shapiro-Wilk normality test\n\ndata:  BMI\nW = 0.82976, p-value = 0.03324\n\n\ninstall.packages(\"nortest\")\n\nlibrary(nortest)\nlillie.test(BMI)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  BMI\nD = 0.27323, p-value = 0.03303\n\n\n\nThe output of the tests of normality above indicate that we should consider bootstrap confidence intervals\n\n\nlibrary(boot)\nmy.mean &lt;- function(x, i){return( mean( x[i] ) )}\nBMI.boot &lt;- boot(BMI, my.mean, 10000)\nboot.ci(BMI.boot, conf=0.95, type=c(\"perc\", \"bca\", \"norm\"))\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 10000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = BMI.boot, conf = 0.95, type = c(\"perc\", \"bca\", \n    \"norm\"))\n\nIntervals : \nLevel      Normal             Percentile            BCa          \n95%   (18.19, 27.01 )   (18.30, 27.10 )   (18.40, 27.20 )  \nCalculations and Intervals on Original Scale\n\n\n\n\nExercise 2\nWhy would we not perform a bootstrap confidence interval on the variable Heightincm in the dataset MA-777 Student Height? Give the standard confidence interval for the mean of student height (use the code t.test(\"variable name\", conf.level=0.95), as used in MA-192).\nHint: Another way to import Excel files into R is using the Import Dataset option.\n\n\nSolution\n\nThe dataset is currently an xlsx file, therefore use either method to import this file. I have used the “Import Dataset” feature.\n\n\nattach(MA_777_Student_Height_1)\n\nThe following object is masked _by_ .GlobalEnv:\n\n    Student\n\n\n\nIn the histogram below I adjust the breaks to make the histogram more presentable\n\n\nhist(`Height in cm`, prob=T, breaks=seq(from=100,to=200, by=10), xlab=\"Height\", main=\"Histogram of MA-777 Student Height\", las=1)\nlines(density(`Height in cm`),col=2)\n\n\n\n\n\n\n\n\n\nNow we perform tests of normality\n\n\nshapiro.test(`Height in cm`)\n\n\n    Shapiro-Wilk normality test\n\ndata:  Height in cm\nW = 0.9614, p-value = 0.2107\n\nlillie.test(`Height in cm`)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  Height in cm\nD = 0.084389, p-value = 0.7082\n\n\n\nThe visualistaions and tests above indicate that the data are normally distributed, therefore we do not use bootstrap methods. Instead we use standard techniques for confidence intervals, see below. The default option to create confidence intervals in R is the t confidence interval.There are a number of ways of doing this including t.test in the package Rmisc.\n\n\nt.test(`Height in cm`, conf.level=0.95)\n\n\n    One Sample t-test\n\ndata:  Height in cm\nt = 77.869, df = 37, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 165.8841 174.7475\nsample estimates:\nmean of x \n 170.3158 \n\n\n\n\nExample 2\nIn this example we will investigate skewness of the variable Salary in the Salary (Skewness) dataset. Follow the steps below:\n\nFirstly, import the SPSS dataset into R and check the first 6 entries:\n\nSalaryData &lt;- read.spss(file.choose(), to.data.frame=T)\nor\n\nSalaryData &lt;- read.spss(\"Salary (Skewness).sav\", to.data.frame=T)\nattach(SalaryData)\nhead(SalaryData)\n\n  Salary\n1  22900\n2  22500\n3  23000\n4  19500\n5  19000\n6  15200\n\n\n\nOne option to investigate skewness is to produce a histogram, but we should first take a look at the data to get a better idea of the range of values and what “breaks” we should use, see below:\n\n\nSalaryData\n\n   Salary\n1   22900\n2   22500\n3   23000\n4   19500\n5   19000\n6   15200\n7   17000\n8   19900\n9   45000\n10  60000\n11  18100\n12  10300\n13  22250\n14  22500\n15  30200\n16  33000\n17  37000\n18  20000\n19  21500\n20  50000\n21  31200\n22  27500\n23  25300\n24  26300\n25  28250\n26   5000\n27   6200\n28  36700\n29  40200\n30  23400\n31  18700\n32  19400\n33  24000\n34  72000\n35  85000\n36   5500\n37   7000\n38  15000\n39  37500\n40  92000\n41  36000\n42  42000\n43  51100\n\nhist(Salary, prob=F, breaks=seq(from=2000,to=100000, by=9000),\n     xlab=\"Salary\", main=\"Histogram of Salary Data\", las=1)\n\n\n\n\n\n\n\n\n\nYou should obtain the above output, indicating left-skewed data.\n\n\n\nNext we calculate the skewness statistic, for which we require the moments package.\n\ninstall.packages(\"moments\")\n\nlibrary(moments)\nskewness(Salary)\n\n[1] 1.515277\n\n\n\nYou should obtain the value 1.515 (to 3dp), as per the output above, providing further evidence of skewness (since the skewness statistic is \\(&gt;1\\)).\n\n\n\nExercise 3\nThe Student Current Account Balance dataset contains the current account balance of 18 students at the end of term. Check the skewness of the variable Balance. Why is a bootstrap confidence interval appropriate in this case? Find a 95% BCa bootstrap confidence interval for the mean of Balance.\nHint: note that the sample size is small. Skewness of data suggests it is not normally distributed, but this can be double-checked using the usual tests.\n\n\nSolution\n\nWe first import the data.\n\nCurrentAccount &lt;- read.spss(file.choose(), to.data.frame=T)\n\nattach(CurrentAccount)\nhead(CurrentAccount)\n\n  Balance\n1    -154\n2    -240\n3     -10\n4       5\n5    -127\n6    -140\n\n\n\nWe now check the skewness and perform tests of normality on the variable Balance\n\n\nskewness(Balance)\n\n[1] 1.082313\n\nshapiro.test(Balance)\n\n\n    Shapiro-Wilk normality test\n\ndata:  Balance\nW = 0.89573, p-value = 0.04845\n\nlibrary(nortest)\nlillie.test(Balance)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  Balance\nD = 0.20631, p-value = 0.04142\n\n\n\nA histogram may also help:\n\n\nhist(Balance, prob=T, breaks=seq(from=-300,to=400, by=50), xlab=\"Balance\", main=\"Histogram of Current Account Balance\", las=1)\nlines(density(Balance),col=2)\n\n\n\n\n\n\n\nBalance\n\n [1] -154 -240  -10    5 -127 -140  100  -55 -143 -132  -80 -165   67 -182 -215\n[16]  210 -253  330\n\n\n\nSince the data does not seem to be normally distributed we perform a bootstrap confidence interval (BCa) as requested in the question.\n\n\nlibrary(boot)\nmy.mean &lt;- function(x, i){return( mean( x[i] ) )}\nBalance.boot &lt;- boot(Balance, my.mean, 10000)\nboot.ci(Balance.boot, conf=0.95, type=c(\"bca\"))\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 10000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = Balance.boot, conf = 0.95, type = c(\"bca\"))\n\nIntervals : \nLevel       BCa          \n95%   (-124.72,   22.24 )  \nCalculations and Intervals on Original Scale\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Labs - Solutions",
      "Lab 1 - Solutions"
    ]
  },
  {
    "objectID": "MA-292 lab 2 site - with solutions.html",
    "href": "MA-292 lab 2 site - with solutions.html",
    "title": "Lab 2 - with solutions",
    "section": "",
    "text": "Please note that there is a file on Canvas called Getting started with R which may be of some use. This provides details of setting up R and Rstudio on your own computer as well as providing an overview of inputting and importing various data files into R. This should mainly serve as a reminder.\nRecall that we can clear the environment using rm(list=ls()) It is advisable to do this before attempting new questions if confusion may arise with variable names etc.\n\nExample 1\nIn this example we will elaborate on one of the lecture examples where we investigated the effectiveness of two different asthma drugs. The data is provided in the Asthma Drugs dataset, and the readings are peak flow meter differences for the two drugs. The dataset must first be adapted into an R friendly form, i.e. create a new column with all the peak flow measurements in one column, which I have called PFD, with an associated grouping variable, which I have called Drug. Remember to save your file as a csv file. Below is a snippet of my revised dataset:\n\nNext, follow the step-by-step guide below,\n\nFirstly, we input the dataset into R, check it using the “head” command (recall this provides the first 6 entries) and attach it.\n\nAsthma&lt;-read.csv(file.choose(), header=T)\nor\n\nAsthma&lt;-read.csv(\"Asthma Drugs Revised.csv\", header=T)\nhead(Asthma)\n\n   A  B  X X.1 PFD Drug\n1  5 30 NA  NA   5    1\n2  7 25 NA  NA   7    1\n3 20  2 NA  NA  20    1\n4  9 30 NA  NA   9    1\n5 25 35 NA  NA  25    1\n6  2 40 NA  NA   2    1\n\nattach(Asthma)\n\n\nIf the two samples are normally distributed then this would be the situation of an independent samples t-test (seen in MA-192). Therefore, we will first check the normality of drugs A and B. To do this, use the following code\n\n\nshapiro.test(A)\n\n\n    Shapiro-Wilk normality test\n\ndata:  A\nW = 0.87786, p-value = 0.1233\n\nshapiro.test(B)\n\n\n    Shapiro-Wilk normality test\n\ndata:  B\nW = 0.82685, p-value = 0.01923\n\n\nand\ninstall.packages(\"nortest\")\n\nlibrary(nortest)\nlillie.test(A)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  A\nD = 0.24576, p-value = 0.08832\n\nlillie.test(B)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  B\nD = 0.24234, p-value = 0.04996\n\n\n\nClearly the data are not normally distributed, therefore we proceed with the non-parametric equivalent test, the Mann-Whitney U-test. Before doing this, we investigate the shape of the distributions of the data. We will do this by creating histograms of the data. We create a factor for Drug and create side-by-side histograms for an easier comparison, noting the choice of “breaks” to provide better presentation, see the code below:\n\n\nDrug&lt;-factor(Drug,c(1,2),labels=c('Drug A','Drug B'))\npar(mfrow=c(1,2)) \nhist(PFD[Drug=='Drug A'], xlab='PFD', main='Histogram for Drug A', breaks=seq(from=0,to=40,by=5)) \nhist(PFD[Drug=='Drug B'], xlab='PFD', main='Histogram for Drug B', breaks=seq(from=0,to=40,by=5))\n\n\n\n\n\n\n\n\n\nThe histograms produced should show that both drugs have similar shaped distributions. Recall from the lectures that this allows us to effectively compare medians using the Mann-Whitney U-test. Without this, we would be testing whether the two groups follow the same distribution.\nTo perform the Mann-Whitney U-test, use the code below (Please note that the default option for pairing is false, i.e. a Mann-Whitney U test and not a Wilcoxon Signed-Rank test):\n\nwilcox.test(PFD~Drug)\n\nYou will have obtain an error concerning ties when you use this approach. For cases where we have ties in the data we have to use the option ``exact=F”, see below:\n\n\nwilcox.test(PFD~Drug, exact=F)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  PFD by Drug\nW = 39, p-value = 0.1739\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nNote that we may use the original data, i.e. side-by-side columns, but we have to adapt the code slightly to:\n\n\nwilcox.test(A,B, exact=F)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  A and B\nW = 39, p-value = 0.1739\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nIn both approaches check that you obtain a p-value of \\(0.1739&gt;0.05\\), therefore we do not reject \\(H_0\\) and conclude that there is no difference between the medians of the two asthma drugs.\nPlease note that there is also the wilcox_test command within the “coin” package that can deal with ties, see\n\ninstall.packages(\"coin\")\n\nlibrary(coin)\n\nLoading required package: survival\n\nwilcox_test(PFD~Drug)\n\n\n    Asymptotic Wilcoxon-Mann-Whitney Test\n\ndata:  PFD by Drug (Drug A, Drug B)\nZ = -1.393, p-value = 0.1636\nalternative hypothesis: true mu is not equal to 0\n\n\n\nWe obtain the same conclusion with this technique.\n\n\n\nExercise 1\nThe Reaction Times dataset contains the results of a trial measuring reaction times of 2 different groups of people, where one group is given water before the trial and the other group is given a new drink designed to reduce reaction times. Perform an appropriate statistical test to determine whether there is any difference in reaction times. Calculate the effect size.\nHint: Note that in order to obtain a \\(Z\\) value to calculate the effect size, the wilcox_test() command from the “coin” package will need to be used. Also remember how to import an SPSS file: either using the foreign package or use the Import Dataset command under Environment.\n\n\nSolutions\n\nImporting the data:\n\nlibrary(foreign)\nReactionData&lt;- read.spss(file.choose(), to.data.frame=T)\n\nWe first investigate the normality of the groups to determine the appropriate test.\n\n\nshapiro.test(ReactionData$NewDrink)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ReactionData$NewDrink\nW = 0.86864, p-value = 0.03223\n\nshapiro.test(ReactionData$Water)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ReactionData$Water\nW = 0.83015, p-value = 0.009204\n\nlibrary(nortest)\nlillie.test(ReactionData$NewDrink)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  ReactionData$NewDrink\nD = 0.22467, p-value = 0.04019\n\nlillie.test(ReactionData$Water)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  ReactionData$Water\nD = 0.23322, p-value = 0.02749\n\n\n\nThe output of the test indicate the groups are not normally distributed, hence we use a Mann-Whitney U test. In order to determine the correct hypotheses we investigate the shape of the groups.\n\n\nDrinkType&lt;-factor(ReactionData$Drink,c(1,2),labels=c('Water','New Drink'))\n\npar(mfrow=c(1,2)) \nhist(ReactionData$ReactionTime[ReactionData$Drink=='Water'], xlab='Reaction', main='Histogram for Water', breaks=seq(from=100,to=400, by=50)) \nhist(ReactionData$ReactionTime[ReactionData$Drink=='New Drink'], xlab='Reaction', main='Histogram for New Drink', breaks=seq(from=100,to=400, by=50))\n\n\n\n\n\n\n\n\n\nThe groups seem to be of a similar shape, hence we compare group medians in the Mann-Whitney U test.\n\n\nwilcox.test(ReactionData$ReactionTime~ReactionData$Drink)\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact p-value with ties\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  ReactionData$ReactionTime by ReactionData$Drink\nW = 1, p-value = 4.012e-06\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nSince there are ties in the dataset we use:\n\n\nwilcox.test(ReactionData$ReactionTime~ReactionData$Drink, exact=F)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  ReactionData$ReactionTime by ReactionData$Drink\nW = 1, p-value = 4.012e-06\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nOr we can use the coin package and use the code below (which provides a Z value that will need to be used to calculate the effect size)\n\n\nlibrary(coin)\nwilcox_test(ReactionData$ReactionTime~ReactionData$Drink)\n\n\n    Asymptotic Wilcoxon-Mann-Whitney Test\n\ndata:  ReactionData$ReactionTime by ReactionData$Drink (New Drink, Water)\nZ = -4.6315, p-value = 3.63e-06\nalternative hypothesis: true mu is not equal to 0\n\n\n\nAs we have a significant result it is good practice to compare medians.\n\n\ntapply(ReactionData$ReactionTime,ReactionData$Drink,median)\n\nNew Drink     Water \n      220       370 \n\n\n\nThe effect size is given by, \\[\nr=\\frac{Z}{\\sqrt{n}},\n\\] where \\(n=n_1+n_2\\).\nIn this exercise, we have \\[\nr=\\frac{-4.632}{\\sqrt{30}}=0.85,\n\\] i.e. a large effect size.\n\n\n\nExample 2\nIn this example we will perform Example 2.9 in R. Using the Public Awareness dataset, we investigate whether there is any difference between a certain video (Video A) and a method using props (Demo D) in informing the public about a certain medical condition. The same participants evaluate each method. Please follow the steps below:\n\nFirstly use the “Import Dataset” command in the “Environment” to import the dataset.\nWe could be mistaken to think that this is a situation where we use a paired t-test. Perform a test of normality on the differences to check one of the main conditions of this test using,\n\n\nshapiro.test(Public_Awareness$Difference)\n\n\n    Shapiro-Wilk normality test\n\ndata:  Public_Awareness$Difference\nW = 0.95299, p-value = 0.4147\n\n\ninstall.packages(\"nortest\")\n\nlibrary(nortest)\nlillie.test(Public_Awareness$Difference)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  Public_Awareness$Difference\nD = 0.15672, p-value = 0.2223\n\n\n\nNote that we do not have evidence to reject the normality of the data. However, if we look carefully at the dataset, we see that we are dealing with ordinal data and therefore the Wilcoxon Signed-Rank test is an option (the assumption of continuous data of the paired t-test is not met since there are only 4 ordered categories - recall that 7 or more are required to be classed as continuous).\nWe next investigate the shape of the differences using a histogram and boxplot in order to determine the correct hypotheses, see\n\n\nhist(Public_Awareness$Difference, xlab='Difference', main='Histogram for Differences', breaks=seq(from=-5,to=10, by=2))\n\n\n\n\n\n\n\nboxplot(Public_Awareness$Difference, main=\"Box Plot of Differences\", ylab=\"Differences\", xlab=\"Video A - Demo D\", las=1)\n\n\n\n\n\n\n\n\n\nFrom your output you should be able to see that the differences do appear to be symmetric.\nNow we proceed to the Wilcoxon Signed-Rank test.\n\n\nwilcox.test(Public_Awareness$TotalAGen, Public_Awareness$TotalDDEMO, paired=TRUE, exact=F)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  Public_Awareness$TotalAGen and Public_Awareness$TotalDDEMO\nV = 167.5, p-value = 0.003558\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nThe test statistic is significant, therefore we conclude that median difference is significantly different to 0.\nThe medians can be calculated using\n\n\nmedian(Public_Awareness$TotalAGen)\n\n[1] 24\n\nmedian(Public_Awareness$TotalDDEMO)\n\n[1] 22\n\n\n\nAs the median of Video A is 24 and that of Demo D is 22, we conclude that Video A seems to be more effective.\nThe coin package has an alternative function for this procedure, however the data must be presented differently, i.e. one column containing the data and a grouping variable. I have created a new file called Public Awareness Revised to do this. Import this dataset and use the relevant code below:\n\n\nwilcoxsign_test(Public_Awareness_Revised$Effectiveness~Public_Awareness_Revised$Method)\n\n\n    Asymptotic Wilcoxon-Pratt Signed-Rank Test\n\ndata:  y by x (pos, neg) \n     stratified by block\nZ = 5.5779, p-value = 2.434e-08\nalternative hypothesis: true mu is not equal to 0\n\n\n\n\nExercise 2\nThe Reorganisation Salaries dataset contains the salaries of individual employees before and after a reorganisation. Investigate whether there is any statistically significant difference in the salaries before and after the reorganisation, justifying fully the procedure used.\nHint: The differences are not provided in the dataset, therefore you must create a new dataset which includes a new column containing the differences. Either do this in Excel before importing the data or use the following code in R:\nSalaries.new &lt;- transform(Salaries, Differences = Before - After)\nSalaries.new\n\n\nSolutions\n\nImporting the data:\n\nSalaries&lt;-read.csv(file.choose(), header=T)\nhead(Salaries)\n\n\n  Before After\n1  20000 25000\n2  36000 45000\n3  26000 19000\n4  26000 14000\n5  73000 89000\n6  34000 23000\n\n\n\nThe differences are not provided in the dataset, therefore we must create a new dataset which includes a new column containing the differences.\n\n\nSalaries.new &lt;- transform(Salaries, Differences = Before - After)\nSalaries.new\n\n   Before  After Differences\n1   20000  25000       -5000\n2   36000  45000       -9000\n3   26000  19000        7000\n4   26000  14000       12000\n5   73000  89000      -16000\n6   34000  23000       11000\n7  149000 147000        2000\n8   66000  50000       16000\n9   23000  24000       -1000\n10  20000  30000      -10000\n11  80000  68000       12000\n12  85000 159000      -74000\n\n\n\nWe next test the normality of the differnecs to determine which technique is suitable.\n\n\nshapiro.test(Salaries.new$Differences)\n\n\n    Shapiro-Wilk normality test\n\ndata:  Salaries.new$Differences\nW = 0.72446, p-value = 0.001465\n\nlibrary(nortest)\nlillie.test(Salaries.new$Differences)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  Salaries.new$Differences\nD = 0.24453, p-value = 0.04588\n\n\n\nSince the differneces do not appear to be normally distributed we procedd with the Wilcoxon Signed-Rank test, however we first determine whether the differences are symmetric.\n\n\nboxplot(Salaries.new$Differences, main=\"Box Plot of Differences\", ylab=\"Differences\", xlab=\"Before - After\", las=1)\n\n\n\n\n\n\n\nhist(Salaries.new$Differences, xlab='Difference', main='Histogram for Differences', breaks=seq(from=-80000,to=40000, by=5000))\n\n\n\n\n\n\n\n\n\nThe differences appear to be symmetric, hence we continue with our test.\n\nwilcox.test(Salaries.new$Before, Salaries.new$After, paired=TRUE)\n\nAs there is a warning about ties, we use:\n\n\nwilcox.test(Salaries.new$Before, Salaries.new$After, paired=TRUE, exact=F)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  Salaries.new$Before and Salaries.new$After\nV = 40.5, p-value = 0.9374\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nNow we see that the result is not significant indicating that there does not appear to be any difference between the median salary before and after the reorganisation.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Labs - Solutions",
      "Lab 2 - Solutions"
    ]
  },
  {
    "objectID": "MA-292 lab 3 site.html",
    "href": "MA-292 lab 3 site.html",
    "title": "Lab 3",
    "section": "",
    "text": "Please note that there is a file on Canvas called Getting started with R which may be of some use. This provides details of setting up R and Rstudio on your own computer as well as providing an overview of inputting and importing various data files into R. This should mainly serve as a reminder.\nRecall that we can clear the environment using rm(list=ls()) It is advisable to do this before attempting new questions if confusion may arise with variable names etc.\n\nExample 1\nIn this example we want to determine whether there is any difference between the driving reaction times after drinking water, coffee and alcohol in the Kruskal-Wallis (Reaction Times) dataset. Please follow the steps below:\n\nFirstly, we input the dataset into R and check it using the ``head” command (recall this provides the first 6 entries). We use the readxl package to import the Excel file.\n\nlibrary(readxl)\nReaction&lt;-read_excel(file.choose())\n\nhead(Reaction)\n\n# A tibble: 6 × 2\n  `Reaction Times` `Drink Groups`\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             0.37              1\n2             0.38              1\n3             0.61              1\n4             0.78              1\n5             0.83              1\n6             0.86              1\n\n\n\nWe next perform a test of normality on the standardized residuals to determine which method should be used. If the residuals are normally distributed and we have homogeneity of variances (Levene’s test) then we would use ANOVA. The following code creates and performs tests of normality on the residuals.\n\n\nanova_model&lt;-aov(Reaction$\"Reaction Times\"~Reaction$\"Drink Groups\")\nsummary(anova_model)\n\n\nresiduals&lt;-rstandard(anova_model)\nshapiro.test(residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals\nW = 0.89219, p-value = 0.005437\n\nlibrary(nortest)\nlillie.test(residuals)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  residuals\nD = 0.14947, p-value = 0.08544\n\n\n\nClearly the residuals are not normally distributed, therefore we must use the nonparametric equivalent of the ANOVA procedure, i.e. the Kruskal-Wallis H-test. In order to determine the correct hypotheses, we next investigate the shapes of the distributions of the groups using histograms:\n\n\nDrinkTypes&lt;-factor(Reaction$\"Drink Groups\",c(1,2,3),labels=c('Water','Coffee', 'Alcohol'))\npar(mfrow=c(1,3)) \nhist(Reaction$\"Reaction Times\"[DrinkTypes=='Water'], xlab='Reaction Time', \n     main='Histogram for Water', breaks=seq(from=0,to=4, by=0.2)) \nhist(Reaction$\"Reaction Times\"[DrinkTypes=='Coffee'], xlab='Reaction Time', \n     main='Histogram for Coffee', breaks=seq(from=0,to=4, by=0.2))\nhist(Reaction$\"Reaction Times\"[DrinkTypes=='Alcohol'], xlab='Reaction Time', \n     main='Histogram for Alcohol', breaks=seq(from=0,to=4, by=0.2))\n\n\n\n\n\nFrom the histograms we see that the groups seem to have similar shaped distributions, therefore use the following hypotheses:\n\n\\(H_0:\\) The medians of all groups are equal;\n\\(H_1:\\) At least one group has median not equal to the others.\n\nNext we perform the Kruskal-Wallis H test.\n\n\nkruskal.test(Reaction$\"Reaction Times\"~Reaction$\"Drink Groups\")\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Reaction$\"Reaction Times\" by Reaction$\"Drink Groups\"\nKruskal-Wallis chi-squared = 16.322, df = 2, p-value = 0.0002856\n\n\n\nAs we have a significant result we undertake a Dunn post hoc test with Bonferroni correction.\n\ninstall.packages(\"FSA\")\n\nlibrary(FSA)\n\n\ndunnTest(Reaction$\"Reaction Times\"~factor(Reaction$\"Drink Groups\"), method=\"bonferroni\")\n\nDunn (1964) Kruskal-Wallis multiple comparison\n\n\n  p-values adjusted with the Bonferroni method.\n\n\n  Comparison         Z      P.unadj        P.adj\n1      1 - 2 -2.095735 3.610569e-02 0.1083170553\n2      1 - 3 -4.039053 5.366736e-05 0.0001610021\n3      2 - 3 -1.943318 5.197773e-02 0.1559332006\n\n\n\nYou should see that there is a significant difference between drinks 1 and 3, i.e. between water and alcohol. We therefore report medians using the following code:\n\n\ntapply(Reaction$\"Reaction Times\",Reaction$\"Drink Groups\",median)\n\n    1     2     3 \n0.845 1.445 2.250 \n\n\n\n\nExercise 1\nThe Diets dataset contains data on the weight loss of 3 groups of participants who took park in 3 different diets. Determine whether there is any difference in the effectiveness of the diets, fully justifying the method used.\n\n\nExample 2\nIn this example we will continue the example of determining whether a bonus scheme improves lateness in employees. Recall the data below:\n\n\n\nEmployee\nBaseline\nMonth 1\nMonth\n\n\n\n\n1\n16\n17\n11\n\n\n2\n10\n5\n2\n\n\n3\n7\n8\n0\n\n\n4\n13\n9\n5\n\n\n5\n17\n2\n2\n\n\n6\n10\n10\n9\n\n\n7\n11\n6\n5\n\n\n\nWe will now use a Friedman test to determine whether there is any difference in the medians of these groups. Please follow the steps below in R:\n\nWe first input the data:\n\n\nBaseline&lt;-c(16,10,7,13,17,10,11)\nMonth1&lt;-c(17,5,8,9,2,10,6)\nMonth2&lt;-c(11,2,0,5,2,9,5)\nLatenessData&lt;-data.frame(Baseline,Month1,Month2)\nLatenessData\n\n  Baseline Month1 Month2\n1       16     17     11\n2       10      5      2\n3        7      8      0\n4       13      9      5\n5       17      2      2\n6       10     10      9\n7       11      6      5\n\n\n\nNext we check the normality of the residuals using the following code. We first need to upload the dataset Work Lateness.csv, which in a different format. (It is not as straightforward as in the Kruskal-Wallis case.)\n\nLatenessNew&lt;-read.csv(file.choose(), header=T)\nLatenessNew\nattach(LatenessNew)\n\nanova_model3 &lt;- aov(Lateness~Bonus+Error(EmployeeA))                                      \nanova.model3.pr &lt;- proj(anova_model3)\nresiduals&lt;-anova.model3.pr[[3]][, \"Residuals\"]\nshapiro.test(residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals\nW = 0.94283, p-value = 0.2478\n\n\n\nThe residuals seem to be normally distributed, however we continue with the Friedman test as the sample size is small (and we will see shortly that the data are skewed). We next investigate the shapes of the group to determine the correct hypotheses.\n\n\npar(mfrow=c(1,3)) \nhist(Baseline, xlab='Late Days', main='Histogram for Baseline', breaks=seq(from=0,to=20, by=2)) \nhist(Month1, xlab='Late Days', main='Histogram for Month 1', breaks=seq(from=0,to=20, by=2))\nhist(Month2, xlab='Late Days', main='Histogram for Month 2', breaks=seq(from=0,to=20, by=2))\n\n\n\n\n\nWe see that the groups seem to have the same shaped distribution, hence we compare medians in the hypotheses. The histograms are all skewed, providing further evidence that non parametric techniques are not suitable. We now proceed with Friedman’s test. Note that we have to make a matrix out of the data to perform the test.\n\n\nLatenessMatrix&lt;-matrix(c(Baseline, Month1, Month2),ncol=3, nrow=7,dimnames = list(1 : 7, c(\"Baseline\", \"Month1\", \"Month2\")))\nLatenessMatrix\n\n  Baseline Month1 Month2\n1       16     17     11\n2       10      5      2\n3        7      8      0\n4       13      9      5\n5       17      2      2\n6       10     10      9\n7       11      6      5\n\nfriedman.test(LatenessMatrix)\n\n\n    Friedman rank sum test\n\ndata:  LatenessMatrix\nFriedman chi-squared = 10.231, df = 2, p-value = 0.006004\n\n\n\nSince we have a significant result we perform a post hoc test. We use the Nemenyi post-hoc tests in the PMCMRplus package.\n\ninstall.packages(\"PMCMRplus\")\n\nlibrary(PMCMRplus)\nkwAllPairsNemenyiTest(LatenessNew$Lateness~LatenessNew$Bonus)\n\n\n    Pairwise comparisons using Tukey-Kramer-Nemenyi all-pairs test with Tukey-Dist approximation\n\n\ndata: LatenessNew$Lateness by LatenessNew$Bonus\n\n\n  0      1     \n1 0.0794 -     \n2 0.0072 0.6646\n\n\n\nP value adjustment method: single-step\n\n\nalternative hypothesis: two.sided\n\n\n\nYou should come to the conclusion from this output that there is a significant difference between Baseline and Month2. We next report their medians using the code:\n\n\nmedian(Baseline)\n\n[1] 11\n\nmedian(Month2)\n\n[1] 5\n\n\n\n\nExercise 2\nDetermine whether there are any differences between the groups Overall score (Video A), Overall score (Video B), Overall score (Video C) and Overall score (Demo D) in the Public Awareness SPSS dataset.\nHint: Before performing the Friedman test you will need to create a matrix of the relevant variables. The following code may help you to do this:\nAwarenessMatrix&lt;-matrix(c(Awareness$TotalAGen, Awareness$TotalBdoc, Awareness$TotalCOld,\n        Awareness$TotalDDEMO), nrow=20,ncol=4, dimnames = list(1 : 20, c(\"A\", \"B\", \"C\", \"D\")))\nYou will then need to create another dataset with one column containing all the awareness scores and a corresponding label column. I would suggest exporting your matrix created above using the write.csv() command and making these changes in Excel, then re-import your new dataset.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "MA-292 lab 4 site.html",
    "href": "MA-292 lab 4 site.html",
    "title": "Lab 4",
    "section": "",
    "text": "Please note that there is a file on Canvas called Getting started with R which may be of some use. This provides details of setting up R and Rstudio on your own computer as well as providing an overview of inputting and importing various data files into R. This should mainly serve as a reminder.\nRecall that we can clear the environment using rm(list=ls()) It is advisable to do this before attempting new questions if confusion may arise with variable names etc.\n\nExample 1\nIn this example we will calculate various probabilities and critical values of chi-square distributions.\n\nFirstly, we let \\(X\\sim\\chi_4^2\\) and we calculate \\(P(X\\leq1.5)\\). Use the following code to calculate this (the help function might help you adapt the code):\n\n\nhelp(\"pchisq\")\npchisq(1.5,4)\n\n[1] 0.1733585\n\n\n\nNext use the following code to calculate \\(P(X\\geq0.25)\\)\n\n\npchisq(0.25,4, lower.tail=F)\n\n[1] 0.992809\n\n\n\nWe now turn to critical values and calculate \\(\\chi_{0.05,9}^2\\)\n\n\nqchisq(0.05,9, lower.tail = F)\n\n[1] 16.91898\n\n\n\nFinally, we calculate \\(\\chi_{0.01,12}^2\\)\n\n\nqchisq(0.01,12, lower.tail = F)\n\n[1] 26.21697\n\n\n\n\nExercise 1\na Let \\(Y\\sim\\chi_7^2\\), calculate \\(P(0.5\\leq Y&lt;5.2)\\). b Calculate the critical values \\(\\chi_{0.05,5}^2\\) and \\(\\chi_{0.005,5}^2\\).\n\n\nExample 2\nIn this example, we will perform the exam classification example, Example 3.1 in the lecture notes, in R. Recall that in this example we want to perform a chi-square goodness-of-fit test on the exams classification dataset below:\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\nObserved (\\(y_i\\))\n32\n48\n71\n30\n19\n\n\nExpected (\\(\\tilde{y}_i\\))\n20\n40\n80\n40\n20\n\n\n\n\nNote: clearly the categories are independent and the expected frequencies are all \\(\\geq5\\), hence satisfying the assumptions for categorical variables.\nWe first input the variables as below:\n\n\nClass&lt;-c(\"A\", \"B\", \"C\", \"D\", \"E\")\nObserved&lt;-c(32,48,71,30,19)\nExpected&lt;-c(20,40,80,40,20)\n\n\nWe require expected proportions and not expected frequencies to perform the chi-square goodness-of-fit test in R, hence we create a new “prop” variable. The code below also creates a dataset from the inputted variables.\n\n\nprop&lt;-Expected/200\nprop\n\n[1] 0.1 0.2 0.4 0.2 0.1\n\nExamClass&lt;-data.frame(Class,Observed,Expected,prop)\n\n\nNow we perform the chi-square goodness-of-fit test.\n\n\nchisq.test(Observed,p=prop)\n\n\n    Chi-squared test for given probabilities\n\ndata:  Observed\nX-squared = 12.363, df = 4, p-value = 0.01485\n\n\n\nYou should obtain a p-value of \\(0.01485&lt;0.05\\) as per the above output, therefore we reject the null hypothesis that the exam results follow the expected distribution. Note that this does not highlight where the discrepancies lie.\nWe next produce side-by-side bar charts to visualise the observed versus expected frequencies. We must first create a matrix containing the observed and expected frequencies, see below.\n\n\nExamClassMatrix&lt;-matrix(c(32,20,48,40,71,80,30,40,19,20), nrow=2,\n              dimnames = list(c(\"Observed\",\"Expected\"), c(\"A\",\"B\",\"C\",\"D\",\"E\")))\nExamClassMatrix\n\n          A  B  C  D  E\nObserved 32 48 71 30 19\nExpected 20 40 80 40 20\n\nbarplot(ExamClassMatrix,\n        beside=TRUE,\n        ylim=c(0, 100),legend=T, col=c(\"yellow\",\"blue\"),\n        xlab=\"Exam Classifications\",\n        ylab=\"Frequency\")\n\n\n\n\n\nThis output echos the results obtained above.\n\n\n\nExercise 2\nThe table below contains the observed and expected number of car insurance claims per policy holder in a given year.\n\n\n\nClaims:\n0\n1\n2\n3\n4\n\n\n\n\n\nObserved\n137\n90\n47\n17\n5\n\n\n\nExpected\n148\n95\n35\n12\n6\n\n\n\n\nChecking the assumptions for categorical variables, perform a \\(\\chi^2\\) goodness-of-fit test on this dataset.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "MA-292 lab 3 site - with solutions.html",
    "href": "MA-292 lab 3 site - with solutions.html",
    "title": "Lab 3 - with solutions",
    "section": "",
    "text": "Please note that there is a file on Canvas called Getting started with R which may be of some use. This provides details of setting up R and Rstudio on your own computer as well as providing an overview of inputting and importing various data files into R. This should mainly serve as a reminder.\nRecall that we can clear the environment using rm(list=ls()) It is advisable to do this before attempting new questions if confusion may arise with variable names etc.\n\nExample 1\nIn this example we want to determine whether there is any difference between the driving reaction times after drinking water, coffee and alcohol in the Kruskal-Wallis (Reaction Times) dataset. Please follow the steps below:\n\nFirstly, we input the dataset into R and check it using the ``head” command (recall this provides the first 6 entries). We use the gdata package to import the Excel file.\n\nlibrary(readxl)\nReaction&lt;-read_excel(file.choose())\n\nhead(Reaction)\n\n# A tibble: 6 × 2\n  `Reaction Times` `Drink Groups`\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             0.37              1\n2             0.38              1\n3             0.61              1\n4             0.78              1\n5             0.83              1\n6             0.86              1\n\n\n\nWe next perform a test of normality on the standardized residuals to determine which method should be used. If the residuals are normally distributed and we have homogeneity of variances (Levene’s test) then we would use ANOVA. The following code creates and performs tests of normality on the residuals.\n\n\nanova_model&lt;-aov(Reaction$\"Reaction Times\"~Reaction$\"Drink Groups\")\nsummary(anova_model)\n\n\nresiduals&lt;-rstandard(anova_model)\nshapiro.test(residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals\nW = 0.89219, p-value = 0.005437\n\nlibrary(nortest)\nlillie.test(residuals)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  residuals\nD = 0.14947, p-value = 0.08544\n\n\n\nClearly the residuals are not normally distributed, therefore we must use the nonparametric equivalent of the ANOVA procedure, i.e. the Kruskal-Wallis H-test. In order to determine the correct hypotheses, we next investigate the shapes of the distributions of the groups using histograms:\n\n\nDrinkTypes&lt;-factor(Reaction$\"Drink Groups\",c(1,2,3),labels=c('Water','Coffee', 'Alcohol'))\npar(mfrow=c(1,3)) \nhist(Reaction$\"Reaction Times\"[DrinkTypes=='Water'], xlab='Reaction Time', \n     main='Histogram for Water', breaks=seq(from=0,to=4, by=0.2)) \nhist(Reaction$\"Reaction Times\"[DrinkTypes=='Coffee'], xlab='Reaction Time', \n     main='Histogram for Coffee', breaks=seq(from=0,to=4, by=0.2))\nhist(Reaction$\"Reaction Times\"[DrinkTypes=='Alcohol'], xlab='Reaction Time', \n     main='Histogram for Alcohol', breaks=seq(from=0,to=4, by=0.2))\n\n\n\n\n\nFrom the histograms we see that the groups seem to have similar shaped distributions, therefore use the following hypotheses:\n\n\\(H_0:\\) The medians of all groups are equal;\n\\(H_1:\\) At least one group has median not equal to the others.\n\nNext we perform the Kruskal-Wallis H test.\n\n\nkruskal.test(Reaction$\"Reaction Times\"~Reaction$\"Drink Groups\")\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Reaction$\"Reaction Times\" by Reaction$\"Drink Groups\"\nKruskal-Wallis chi-squared = 16.322, df = 2, p-value = 0.0002856\n\n\n\nAs we have a significant result we undertake a Dunn post hoc test with Bonferroni correction.\n\ninstall.packages(\"FSA\")\n\nlibrary(FSA)\n\n\ndunnTest(Reaction$\"Reaction Times\"~factor(Reaction$\"Drink Groups\"), method=\"bonferroni\")\n\nDunn (1964) Kruskal-Wallis multiple comparison\n\n\n  p-values adjusted with the Bonferroni method.\n\n\n  Comparison         Z      P.unadj        P.adj\n1      1 - 2 -2.095735 3.610569e-02 0.1083170553\n2      1 - 3 -4.039053 5.366736e-05 0.0001610021\n3      2 - 3 -1.943318 5.197773e-02 0.1559332006\n\n\n\nYou should see that there is a significant difference between drinks 1 and 3, i.e. between water and alcohol. We therefore report medians using the following code:\n\n\ntapply(Reaction$\"Reaction Times\",Reaction$\"Drink Groups\",median)\n\n    1     2     3 \n0.845 1.445 2.250 \n\n\n\n\nExercise 1\nThe Diets dataset contains data on the weight loss of 3 groups of participants who took park in 3 different diets. Determine whether there is any difference in the effectiveness of the diets, fully justifying the method used.\n\n\nSolution\n\nWe first import the data.\n\nDiets&lt;-read.csv(file.choose(), header = T)\n\nIn order to determine whether we need to use a parametric or non-parametric technique, we investigate the normality of the residuals.\n\n\nanova_model2&lt;-aov(Diets$Weight.Loss~Diets$Diet)\nsummary(anova_model2)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\nDiets$Diet   1   12.0   12.00   0.369  0.552\nResiduals   16  520.9   32.56               \n\nresiduals2&lt;-rstandard(anova_model2)\nshapiro.test(residuals2)\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals2\nW = 0.89376, p-value = 0.04479\n\nlibrary(nortest)\nlillie.test(residuals2)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  residuals2\nD = 0.27639, p-value = 0.00078\n\n\n\nThe results indicate that the residuals do not appear to be normally distributed, hence we use the Kruskal-Wallis test (as we have 3 independent groups).\nIn order to identify the correct hypotheses, we need to investigate the shape of the distributions of the groups.\n\n\npar(mfrow=c(1,3)) \nhist(Diets$Weight.Loss[Diets$Diet==1], xlab='Weight Loss', main='Histogram for Diet 1', breaks=seq(from=-10,to=10, by=2)) \nhist(Diets$Weight.Loss[Diets$Diet==2], xlab='Weight Loss', main='Histogram for Diet 2', breaks=seq(from=-10,to=10, by=2))\nhist(Diets$Weight.Loss[Diets$Diet==3], xlab='Weight Loss', main='Histogram for Diet 3', breaks=seq(from=-10,to=10, by=2))\n\n\n\n\n\nFrom the histograms we see that the groups seem to have similar shaped distributions, therefore we will compare medians in the Kruskal-Wallis test.\n\n\nkruskal.test(Diets$Weight.Loss~Diets$Diet)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Diets$Weight.Loss by Diets$Diet\nKruskal-Wallis chi-squared = 6.5272, df = 2, p-value = 0.03825\n\n\n\nAs we have a significant result we undertake a Dunn post hoc test with Bonferroni correction.\n\ninstall.packages(\"FSA\")\n\nlibrary(FSA)\ndunnTest(Diets$Weight.Loss~Diets$Diet, method=\"bonferroni\")\n\nDunn (1964) Kruskal-Wallis multiple comparison\n\n\n  p-values adjusted with the Bonferroni method.\n\n\n  Comparison         Z    P.unadj      P.adj\n1      1 - 2 -1.306966 0.19122413 0.57367240\n2      1 - 3  1.075422 0.28218581 0.84655744\n3      2 - 3  2.554352 0.01063857 0.03191572\n\n\n\nThis shows a significant difference between diest 1 and 2. We therefore report the medians of these groups.\n\n\ntapply(Diets$Weight.Loss,Diets$Diet,median)\n\n  1   2   3 \n3.0 7.5 2.0 \n\n\n\n\nExample 2\nIn this example we will continue the example of determining whether a bonus scheme improves lateness in employees. Recall the data below:\n\n\n\nEmployee\nBaseline\nMonth 1\nMonth\n\n\n\n\n1\n16\n17\n11\n\n\n2\n10\n5\n2\n\n\n3\n7\n8\n0\n\n\n4\n13\n9\n5\n\n\n5\n17\n2\n2\n\n\n6\n10\n10\n9\n\n\n7\n11\n6\n5\n\n\n\nWe will now use a Friedman test to determine whether there is any difference in the medians of these groups. Please follow the steps below in R:\n\nWe first input the data:\n\n\nBaseline&lt;-c(16,10,7,13,17,10,11)\nMonth1&lt;-c(17,5,8,9,2,10,6)\nMonth2&lt;-c(11,2,0,5,2,9,5)\nLatenessData&lt;-data.frame(Baseline,Month1,Month2)\nLatenessData\n\n  Baseline Month1 Month2\n1       16     17     11\n2       10      5      2\n3        7      8      0\n4       13      9      5\n5       17      2      2\n6       10     10      9\n7       11      6      5\n\n\n\nNext we check the normality of the residuals using the following code. We first need to upload the dataset Work Lateness.csv, which in a different format. (It is not as straightforward as in the Kruskal-Wallis case.)\n\nLatenessNew&lt;-read.csv(file.choose(), header=T)\nLatenessNew\nattach(LatenessNew)\n\nanova_model3 &lt;- aov(Lateness~Bonus+Error(EmployeeA))                                      \nanova.model3.pr &lt;- proj(anova_model3)\nresiduals&lt;-anova.model3.pr[[3]][, \"Residuals\"]\nshapiro.test(residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals\nW = 0.94283, p-value = 0.2478\n\n\n\nThe residuals seem to be normally distributed, however we continue with the Friedman test as the sample size is small (and we will see shortly that the data are skewed). We next investigate the shapes of the group to determine the correct hypotheses.\n\n\npar(mfrow=c(1,3)) \nhist(Baseline, xlab='Late Days', main='Histogram for Baseline', breaks=seq(from=0,to=20, by=2)) \nhist(Month1, xlab='Late Days', main='Histogram for Month 1', breaks=seq(from=0,to=20, by=2))\nhist(Month2, xlab='Late Days', main='Histogram for Month 2', breaks=seq(from=0,to=20, by=2))\n\n\n\n\n\nWe see that the groups seem to have the same shaped distribution, hence we compare medians in the hypotheses. The histograms are all skewed, providing further evidence that non parametric techniques are not suitable. We now proceed with Friedman’s test. Note that we have to make a matrix out of the data to perform the test.\n\n\nLatenessMatrix&lt;-matrix(c(Baseline, Month1, Month2),ncol=3, nrow=7,dimnames = list(1 : 7, c(\"Baseline\", \"Month1\", \"Month2\")))\nLatenessMatrix\n\n  Baseline Month1 Month2\n1       16     17     11\n2       10      5      2\n3        7      8      0\n4       13      9      5\n5       17      2      2\n6       10     10      9\n7       11      6      5\n\nfriedman.test(LatenessMatrix)\n\n\n    Friedman rank sum test\n\ndata:  LatenessMatrix\nFriedman chi-squared = 10.231, df = 2, p-value = 0.006004\n\n\n\nSince we have a significant result we perform a post hoc test. We use the Nemenyi post-hoc tests in the PMCMRplus package.\n\ninstall.packages(\"PMCMRplus\")\n\nlibrary(PMCMRplus)\nkwAllPairsNemenyiTest(LatenessNew$Lateness~LatenessNew$Bonus)\n\n\n    Pairwise comparisons using Tukey-Kramer-Nemenyi all-pairs test with Tukey-Dist approximation\n\n\ndata: LatenessNew$Lateness by LatenessNew$Bonus\n\n\n  0      1     \n1 0.0794 -     \n2 0.0072 0.6646\n\n\n\nP value adjustment method: single-step\n\n\nalternative hypothesis: two.sided\n\n\n\nYou should come to the conclusion from this output that there is a significant difference between Baseline and Month2. We next report their medians using the code:\n\n\nmedian(Baseline)\n\n[1] 11\n\nmedian(Month2)\n\n[1] 5\n\n\n\n\nExercise 2\nDetermine whether there are any differences between the groups Overall score (Video A), Overall score (Video B), Overall score (Video C) and Overall score (Demo D) in the Public Awareness SPSS dataset.\nHint: Before performing the Friedman test you will need to create a matrix of the relevant variables. The following code may help you to do this:\nAwarenessMatrix&lt;-matrix(c(Awareness$TotalAGen, Awareness$TotalBdoc, Awareness$TotalCOld,\n        Awareness$TotalDDEMO), nrow=20,ncol=4, dimnames = list(1 : 20, c(\"A\", \"B\", \"C\", \"D\")))\nYou will then need to create another dataset with one column containing all the awareness scores and a corresponding label column. I would suggest exporting your matrix created above using the write.csv() command and making these changes in Excel, then re-import your new dataset.\n\n\nSolution\n\nImport the data:\n\nlibrary(foreign)\nAwareness&lt;-read.spss(file.choose(),to.data.frame = T)\nAwareness\n\nWe assume that the residuals are not normally distributed.\nWe next investigate the shapes of the group to determine the correct hypotheses.\n\n\npar(mfrow=c(1,4)) \nhist(Awareness$TotalAGen, xlab='Overall Score', main='Histogram for Video A', breaks=seq(from=5,to=30, by=4)) \nhist(Awareness$TotalBdoc, xlab='Overall Score', main='Histogram for Video B', breaks=seq(from=5,to=30, by=4))\nhist(Awareness$TotalCOld, xlab='Overall Score', main='Histogram for Video C', breaks=seq(from=5,to=30, by=4))\nhist(Awareness$TotalDDEMO, xlab='Overall Score', main='Histogram for Demo D', breaks=seq(from=5,to=30, by=4))\n\n\n\n\n\nThe histograms show reasonably similar distribution shape, with there being a clear right skew for Video A, Video B and Demo D. We will therefore compare medians in the hypotheses for the Friedman test.\n\n\nAwarenessMatrix&lt;-matrix(c(Awareness$TotalAGen, Awareness$TotalBdoc, Awareness$TotalCOld,\n        Awareness$TotalDDEMO), nrow=20,ncol=4, dimnames = list(1 : 20, c(\"A\", \"B\", \"C\", \"D\")))\n\nAwarenessMatrix\n\n    A  B  C  D\n1  25 23 13 22\n2  23 23 20 22\n3  20 17 14 23\n4  24 24 23 25\n5  25 22 18 23\n6  24 22 23 25\n7  25 22  9 21\n8  24 23  6 21\n9  25 21 11 23\n10 24 23  5 25\n11 24 23 15 21\n12 22 24 14 19\n13 24 23 14 19\n14 25 25 17 18\n15 24 22 14 17\n16 25 23 13 20\n17 25 22 16 21\n18 25 21 11 22\n19 25 23 19 25\n20 24 23 18 25\n\nfriedman.test(AwarenessMatrix)\n\n\n    Friedman rank sum test\n\ndata:  AwarenessMatrix\nFriedman chi-squared = 41.372, df = 3, p-value = 5.452e-09\n\n\n\nClearly we have a significant result, therefore we proceed with post hoc testing.\nNow a new data set is required, i.e. in a different format. We first export the matrix, create a new column containing the Awareness variable with a labelling column\n\nwrite.csv(AwarenessMatrix, \"Awareness.csv\")\n\nMake the changes and re-import the new dataset.\n\nAwarenessPostHoc&lt;-read.csv(file.choose())\n\nWe now perform the post hoc testing.\n\ninstall.packages(\"PMCMRplus\")\n\nlibrary(PMCMRplus)\nkwAllPairsNemenyiTest(AwarenessPostHoc$Awareness~AwarenessPostHoc$Group)\n\n\n    Pairwise comparisons using Tukey-Kramer-Nemenyi all-pairs test with Tukey-Dist approximation\n\n\ndata: AwarenessPostHoc$Awareness by AwarenessPostHoc$Group\n\n\n  1      2      3     \n2 0.0562 -      -     \n3 3e-10  0.0003 -     \n4 0.0226 0.9880 0.0011\n\n\n\nP value adjustment method: single-step\n\n\nalternative hypothesis: two.sided\n\n\n\nThis shows significant differences between Video C and Videos A and B and between Demo D and Video C.\nWe therefore report medians:\n\n\nmedian(Awareness$TotalAGen)\n\n[1] 24\n\nmedian(Awareness$TotalBdoc)\n\n[1] 23\n\nmedian(Awareness$TotalCOld)\n\n[1] 14\n\nmedian(Awareness$TotalDDEMO)\n\n[1] 22\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "MA-292 lab 5 site.html",
    "href": "MA-292 lab 5 site.html",
    "title": "Lab 5",
    "section": "",
    "text": "Please note that there is a file on Canvas called Getting started with R which may be of some use. This provides details of setting up R and Rstudio on your own computer as well as providing an overview of inputting and importing various data files into R. This should mainly serve as a reminder.\nRecall that we can clear the environment using rm(list=ls()) It is advisable to do this before attempting new questions if confusion may arise with variable names etc.\n\nExample 1\nIn this example, we will investigate whether there is an association between personality and colour preference. This is an elaboration on Example 3.5 in the lecture notes using R. Here is the data:\n\n\n\nPersonality\nRed\nYellow\nGreen\nBlue\nTotals\n\n\n\n\nIntrovert\n20\n6\n30\n44\n100\n\n\nExtrovert\n180\n34\n50\n36\n300\n\n\nTotals\n200\n40\n80\n80\n400\n\n\n\nFollow the step-by-step guide below:\n\nFirstly, we input the data and create a matrix.\n\n\nR1&lt;-c(20,6,30,44)\nR2&lt;-c(180,34,50,36)\nPersonalityMatrix&lt;-matrix(c(R1,R2),nrow=2,byrow=T,\n      dimnames = list(c(\"Introvert\", \"Extrovert\"),c(\"Red\", \"Yellow\", \"Green\", \"Blue\")))\nPersonalityMatrix\n\n          Red Yellow Green Blue\nIntrovert  20      6    30   44\nExtrovert 180     34    50   36\n\n\n\nWe next visualise the data using side-by-side bar charts.\n\n\nbarplot(PersonalityMatrix,\n        beside=TRUE,\n        ylim=c(0, 200),legend=T, col=c(\"yellow\",\"blue\"),\n        xlab=\"Colour Preference\",\n        ylab=\"Frequency\")\n\n\n\n\n\nOn the face of it, it looks like a higher proportion of Extroverts prefer the colour red, but we will have to now proceed with a statistical approach, i.e. a chi-square test of association using the following code:\n\n\nresult&lt;-chisq.test(PersonalityMatrix)\nresult\n\n\n    Pearson's Chi-squared test\n\ndata:  PersonalityMatrix\nX-squared = 71.2, df = 3, p-value = 2.362e-15\n\n\n\nIn terms of the assumptions, we can check independence by inspection and we check the expected frequencies using the following code:\n\n\nresult$expected\n\n          Red Yellow Green Blue\nIntrovert  50     10    20   20\nExtrovert 150     30    60   60\n\n\n\nThe output shows no expected frequency is less than 5, satisfying the assumption.\nReturning to the chi-square test output, we see that we have a significant result, i.e. we reject the null hypothesis that there is no association and conclude that there appears to be an association between personality and colour preference. Next we calculate the effect size. For this we need the “vcd” package in R, see below:\n\ninstall.packages(\"vcd\")\n\nlibrary(vcd)\nassocstats(PersonalityMatrix)\n\n                    X^2 df   P(&gt; X^2)\nLikelihood Ratio 70.066  3 4.1078e-15\nPearson          71.200  3 2.3315e-15\n\nPhi-Coefficient   : NA \nContingency Coeff.: 0.389 \nCramer's V        : 0.422 \n\n\n\nCramer’s V of 0.422 indicates a medium to strong effect. Finally, we perform post hoc testing using the standardized residuals approach.\n\ninstall.packages(\"questionr\")\n\nlibrary(questionr)\nchisq.residuals(PersonalityMatrix)\n\n            Red Yellow Green  Blue\nIntrovert -4.24  -1.26  2.24  5.37\nExtrovert  2.45   0.73 -1.29 -3.10\n\n\n\nWe can think of the standardized residuals as \\(z\\)-scores, therefore, we look for \\[\n|\\text{standardized  residual}|&gt;1.96\n\\] to be significant at the 5% level. Therefore, the values -3.1 and 2.45 are significant for Extrovert, which indicates that fewer extroverts prefer blue and more extroverts prefer red. The values 5.37 and -4.24 for Introvert are also significant, indicating that more introverts prefer blue, but fewer introverts prefer red.\n\n\n\nExercise 1\nInvestigate, checking the relevant assumptions, whether there is any relationship between Male and Female staff and Earnings based on the data provided by a local company below:\n\n\n\n\nMale\nFemale\n\n\n\n\nHigh Earnings\n37\n39\n\n\nLow Earnings\n25\n23\n\n\n\nIf there is an association investigate this.\nHint: Clearly this is a \\(2\\times2\\) contingency table and hence we must use Yates’ continuity correction (checking assumptions). In R, this is performed using the correct=T option of the chi-square test, see below:\nresult2&lt;-chisq.test(EarningsMatrix, correct=T)\nresult2\n\n\nExample 2\nThis example will look at using the linear-by-linear option when testing for association between ordinal variables (see Remark 3.3 in the lecture notes). Independence may be assumed, but check the expected frequencies assumption. We will investigate whether there is an association between education levels and where people live using the following fictitious dataset.\n\n\n\n\nGCSE\nA-level\nDegree\n\n\n\n\nVillage\n130\n50\n20\n\n\nCity\n40\n50\n110\n\n\n\nFollow the steps below in R.\n\nTask: input the variables and create a matrix called EducationMatrix.\nCheck your matrix agains the output below:\n\n\n\n        GCSE A-level Degree\nVillage  130      50     20\nCity      40      50    110\n\n\n\nNext we visualise the data using bar charts:\n\n\nbarplot(EducationMatrix,\n        beside=TRUE,\n        ylim=c(0, 170),legend=T, col=c(\"yellow\",\"blue\"),\n        xlab=\"Education\",\n        ylab=\"Frequency\")\n\n\n\n\n\nNext we perform the Chi-square test of association in order to check the expected frequencies assumption. (Independence is clear by inspection.)\n\n\nresult3&lt;-chisq.test(EducationMatrix)\nresult3\n\n\n    Pearson's Chi-squared test\n\ndata:  EducationMatrix\nX-squared = 109.95, df = 2, p-value &lt; 2.2e-16\n\nresult3$expected\n\n        GCSE A-level Degree\nVillage   85      50     65\nCity      85      50     65\n\n\n\nNote that no expected frequency is \\(&lt;5\\). We now perform the linear-by-linear test (as we have an ordinal variable) using the code below. Note that we first need to covret the data into “table” form.\n\n\ntab&lt;-as.table(EducationMatrix)\ntab\n\n        GCSE A-level Degree\nVillage  130      50     20\nCity      40      50    110\n\nlibrary(coin)\nresult4&lt;-lbl_test(tab)\nresult4\n\n\n    Asymptotic Linear-by-Linear Association Test\n\ndata:  Var2 (ordered) by Var1 (Village, City)\nZ = -10.449, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\nThis result is significant, hence there appears to be an association between location and educational levels. Next we calculate Cramer’s V:\n\n\nlibrary(vcd)\nassocstats(EducationMatrix)\n\n                    X^2 df P(&gt; X^2)\nLikelihood Ratio 118.76  2        0\nPearson          109.95  2        0\n\nPhi-Coefficient   : NA \nContingency Coeff.: 0.464 \nCramer's V        : 0.524 \n\n\n\nAs we have a significant result, we finally perform a post hoc test.\n\n\nlibrary(questionr)\nchisq.residuals(EducationMatrix)\n\n         GCSE A-level Degree\nVillage  4.88       0  -5.58\nCity    -4.88       0   5.58\n\n\n\nThis together with the bar charts indicates that more people in cities have a degree as their highest qualification, while more people in the villages have GCSEs as their highest qualification.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "MA-292 lab 4 site - with solutions.html",
    "href": "MA-292 lab 4 site - with solutions.html",
    "title": "Lab 4 - with solutions",
    "section": "",
    "text": "Please note that there is a file on Canvas called Getting started with R which may be of some use. This provides details of setting up R and Rstudio on your own computer as well as providing an overview of inputting and importing various data files into R. This should mainly serve as a reminder.\nRecall that we can clear the environment using rm(list=ls()) It is advisable to do this before attempting new questions if confusion may arise with variable names etc.\n\nExample 1\nIn this example we will calculate various probabilities and critical values of chi-square distributions.\n\nFirstly, we let \\(X\\sim\\chi_4^2\\) and we calculate \\(P(X\\leq1.5)\\). Use the following code to calculate this (the help function might help you adapt the code):\n\n\nhelp(\"pchisq\")\npchisq(1.5,4)\n\n[1] 0.1733585\n\n\n\nNext use the following code to calculate \\(P(X\\geq0.25)\\)\n\n\npchisq(0.25,4, lower.tail=F)\n\n[1] 0.992809\n\n\n\nWe now turn to critical values and calculate \\(\\chi_{0.05,9}^2\\)\n\n\nqchisq(0.05,9, lower.tail = F)\n\n[1] 16.91898\n\n\n\nFinally, we calculate \\(\\chi_{0.01,12}^2\\)\n\n\nqchisq(0.01,12, lower.tail = F)\n\n[1] 26.21697\n\n\n\n\nExercise 1\na Let \\(Y\\sim\\chi_7^2\\), calculate \\(P(0.5\\leq Y&lt;5.2)\\). b Calculate the critical values \\(\\chi_{0.05,5}^2\\) and \\(\\chi_{0.005,5}^2\\).\n\n\nSolutions\n\n\\(P(0.5\\leq Y&lt;5.2)\\):\n\n\npchisq(5.2,7)-pchisq(0.5,7)\n\n[1] 0.3638756\n\n\n\nCritical values for Chi-square, 5 df, \\(\\alpha=0.05\\) and \\(\\alpha=0.005\\):\n\n\nqchisq(0.05,5, lower.tail = F)\n\n[1] 11.0705\n\nqchisq(0.005,5, lower.tail = F)\n\n[1] 16.7496\n\n\n\n\nExample 2\nIn this example, we will perform the exam classification example, Example 3.1 in the lecture notes, in R. Recall that in this example we want to perform a chi-square goodness-of-fit test on the exams classification dataset below:\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\nObserved (\\(y_i\\))\n32\n48\n71\n30\n19\n\n\nExpected (\\(\\tilde{y}_i\\))\n20\n40\n80\n40\n20\n\n\n\n\nNote: clearly the categories are independent and the expected frequencies are all \\(\\geq5\\), hence satisfying the assumptions for categorical variables.\nWe first input the variables as below:\n\n\nClass&lt;-c(\"A\", \"B\", \"C\", \"D\", \"E\")\nObserved&lt;-c(32,48,71,30,19)\nExpected&lt;-c(20,40,80,40,20)\n\n\nWe require expected proportions and not expected frequencies to perform the chi-square goodness-of-fit test in R, hence we create a new “prop” variable. The code below also creates a dataset from the inputted variables.\n\n\nprop&lt;-Expected/200\nprop\n\n[1] 0.1 0.2 0.4 0.2 0.1\n\nExamClass&lt;-data.frame(Class,Observed,Expected,prop)\n\n\nNow we perform the chi-square goodness-of-fit test.\n\n\nchisq.test(Observed,p=prop)\n\n\n    Chi-squared test for given probabilities\n\ndata:  Observed\nX-squared = 12.363, df = 4, p-value = 0.01485\n\n\n\nYou should obtain a p-value of \\(0.01485&lt;0.05\\) as per the above output, therefore we reject the null hypothesis that the exam results follow the expected distribution. Note that this does not highlight where the discrepancies lie.\nWe next produce side-by-side bar charts to visualise the observed versus expected frequencies. We must first create a matrix containing the observed and expected frequencies, see below.\n\n\nExamClassMatrix&lt;-matrix(c(32,20,48,40,71,80,30,40,19,20), nrow=2,\n              dimnames = list(c(\"Observed\",\"Expected\"), c(\"A\",\"B\",\"C\",\"D\",\"E\")))\nExamClassMatrix\n\n          A  B  C  D  E\nObserved 32 48 71 30 19\nExpected 20 40 80 40 20\n\nbarplot(ExamClassMatrix,\n        beside=TRUE,\n        ylim=c(0, 100),legend=T, col=c(\"yellow\",\"blue\"),\n        xlab=\"Exam Classifications\",\n        ylab=\"Frequency\")\n\n\n\n\n\nThis output echos the results obtained above.\n\n\n\nExercise 2\nThe table below contains the observed and expected number of car insurance claims per policy holder in a given year.\n\n\n\nClaims:\n0\n1\n2\n3\n4\n\n\n\n\n\nObserved\n137\n90\n47\n17\n5\n\n\n\nExpected\n148\n95\n35\n12\n6\n\n\n\n\nChecking the assumptions for categorical variables, perform a \\(\\chi^2\\) goodness-of-fit test on this dataset.\n\n\nSolution\n\nWe first enter the data\n\n\nClaims&lt;-c(0,1,2,3,4)\nObserved2&lt;-c(137,90,47,17,5)\nExpected2&lt;-c(148,95,35,12,6)\n\n\nWe require expected proportions and not expected frequencies in the R chi-square test, hence we create a new “prop” variable\n\n\nprop2&lt;-Expected2/296\n\n\nHere we create the dataset.\n\n\nClaimsData&lt;-data.frame(Claims,Observed2,Expected2, prop2)\nClaimsData\n\n  Claims Observed2 Expected2      prop2\n1      0       137       148 0.50000000\n2      1        90        95 0.32094595\n3      2        47        35 0.11824324\n4      3        17        12 0.04054054\n5      4         5         6 0.02027027\n\n\n\nNext we perform the chi-square goodness-of-fit test\n\n\nchisq.test(Observed2,p=prop2)\n\n\n    Chi-squared test for given probabilities\n\ndata:  Observed2\nX-squared = 7.445, df = 4, p-value = 0.1142\n\n\n\nThe p-value is 0.1142 therefore we do not reject the null hypothesis and conclude that the data is as expected.\nFinally, we produce bar charts of the observed and expected frequencies for each claims number.\n\n\nClaimsMatrix&lt;-matrix(c(137,148,90,95,47,35,17,12,5,6), nrow=2, dimnames = list(c(\"Observed\",\"Expected\"), c(\"0\",\"1\",\"2\",\"3\",\"4\")))\nClaimsMatrix\n\n           0  1  2  3 4\nObserved 137 90 47 17 5\nExpected 148 95 35 12 6\n\nbarplot(ClaimsMatrix,\n        beside=TRUE,\n        ylim=c(0, 200),legend=T, col=c(\"yellow\",\"blue\"),\n        xlab=\"No. of Claims\",\n        ylab=\"Frequency\")\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "MA-292 lab 6 site.html",
    "href": "MA-292 lab 6 site.html",
    "title": "Lab 6",
    "section": "",
    "text": "Please note that there is a file on Canvas called Getting started with R which may be of some use. This provides details of setting up R and Rstudio on your own computer as well as providing an overview of inputting and importing various data files into R. This should mainly serve as a reminder.\nRecall that we can clear the environment using rm(list=ls()) It is advisable to do this before attempting new questions if confusion may arise with variable names etc.\n\nExample 1\nIn this example we will see how to calculate probabilities from the hypergeometric distribution.\n\nWe will calculate the probability as seen in Example 3.8 in the lecture notes. Recall Example 3.8:\n\n“Suppose we have a box of 20 marbles in which 8 are blue and 12 are red. Ten marbles are taken from the box without replacement. Find the probability of obtaining exactly 3 blue marbles.”\n\nIn this case, let \\(X\\) denote the number of blue marbles, i.e. \\(X\\sim\\text{hypergeometric}(20, 8, x)\\) and therefore we want to find \\(P(X=3)\\). We further will calculate \\(P(X\\leq 2)\\) and \\(P(X\\geq 6)\\). Use the help function to understand the parameters.\n\n\nhelp(\"dhyper\")\ndhyper(3,8,12,10)\n\n[1] 0.2400572\n\nphyper(2,8,12,10)\n\n[1] 0.08490117\n\nphyper(5,8,12,10,lower.tail = F)\n\n[1] 0.08490117\n\n\n\n\nExercise 1\nIf we add 5 blue marbles to the box in the example above, but keep the sample the same (10), what is \\(P(X&gt;4)\\)?\n\n\nExercise 2\nSuppose we now have a situation where we have 10 green and 20 white marbles and 15 are selected. Let \\(Y\\) denote the number of green marbles selected, i.e. \\[Y\\sim\\text{hypergeometric}(30,10,y).\\] Calculate \\(P(Y=5)\\) and \\(P(Y&lt;3)\\).\n\n\nExample 2\nIn this example we will see how to use Fisher’s Exact Test in R. Firstly, remind yourself why we would use this test over the \\(\\chi^2\\) test - see the lecture notes or full details. This is important when interpreting the R output.\nWe will consider Example 3.9 in the lecture notes where we investigate whether there is a relationship between attendance and exam results. Clearly we have independence, i.e. each member can only contribute to one entry in the table:\n\n\n\n\nPass\nFail\nTotals\n\n\n\n\nAttendance \\(\\mathbf{&gt;50\\%}\\)\n10\n2\n12\n\n\nAttendance \\(\\mathbf{\\leq50\\%}\\)\n5\n7\n12\n\n\nTotals\n15\n9\n24\n\n\n\n\nRecall the hypotheses:\n\n\\(H_0\\): there is no association between attendance and passing the exam;\n\\(H_1\\): if attendance \\(&gt;50\\%\\) students are more likely to pass (one-tailed).\n\nFirstly input the data into R and create a matrix:\n\n\nRow1&lt;-c(10,2)\nRow2&lt;-c(5,7)\n\nAttendanceMatrix&lt;-matrix(c(Row1,Row2), byrow=T,nrow=2,\n          dimnames=list(c(\"&gt;50\",\"50 or less\"),c(\"Pass\",\"Fail\")))\nAttendanceMatrix\n\n           Pass Fail\n&gt;50          10    2\n50 or less    5    7\n\n\n\nNext we visualise the data to get a feeling for potential relationships.\n\n\nbarplot(AttendanceMatrix,\n        beside=TRUE,\n        ylim=c(0, 30),legend=T, col=c(\"yellow\",\"blue\"),\n        xlab=\"Pass/Fail\",\n        ylab=\"Frequency\")\n\n\n\n\n\nNext we perform the Chi-square test of association, but we will see that the expected frequency assumption fails:\n\n\nresult&lt;-chisq.test(AttendanceMatrix, correct=T)\nresult\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  AttendanceMatrix\nX-squared = 2.8444, df = 1, p-value = 0.09169\n\nresult$expected\n\n           Pass Fail\n&gt;50         7.5  4.5\n50 or less  7.5  4.5\n\n\n\nAs the expected frequency assumption fails (we have independence by inspection) we now proceed with Fisher’s exact test. The help function should be used to make sure we perform an appropriate one-tailed test.\n\n\nhelp(\"fisher.test\")\nfisher.test(AttendanceMatrix, alternative = \"greater\")\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  AttendanceMatrix\np-value = 0.04469\nalternative hypothesis: true odds ratio is greater than 1\n95 percent confidence interval:\n 1.044487      Inf\nsample estimates:\nodds ratio \n  6.394505 \n\n\n\nLooking at the statistic for this test in the output tables, we see that we have a p-value of \\(0.045&lt;0.05\\), therefore we reject \\(H_0\\) and conclude that evidence suggests that if students attend more than 50% of lectures then they are more likely to pass the exam. This result confirms our manual calculations in the lecture.\n\n\n\nExercise 3\nThe table below contains information on the number of males and females earning low, medium and high salaries in a particular institution.\n\n\n\n\nLow\nMedium\nHigh\nTotals\n\n\n\n\nMale\n6\n58\n3\n67\n\n\nFemale\n3\n44\n10\n57\n\n\nTotals\n9\n102\n13\n\n\n\n\nInvestigate whether there is an association between salary level and gender, justifying the method you use.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "MA-292 lab 5 site - with solutions.html",
    "href": "MA-292 lab 5 site - with solutions.html",
    "title": "Lab 5 - with solutions",
    "section": "",
    "text": "Please note that there is a file on Canvas called Getting started with R which may be of some use. This provides details of setting up R and Rstudio on your own computer as well as providing an overview of inputting and importing various data files into R. This should mainly serve as a reminder.\nRecall that we can clear the environment using rm(list=ls()) It is advisable to do this before attempting new questions if confusion may arise with variable names etc.\n\nExample 1\nIn this example, we will investigate whether there is an association between personality and colour preference. This is an elaboration on Example 3.5 in the lecture notes using R. Here is the data:\n\n\n\nPersonality\nRed\nYellow\nGreen\nBlue\nTotals\n\n\n\n\nIntrovert\n20\n6\n30\n44\n100\n\n\nExtrovert\n180\n34\n50\n36\n300\n\n\nTotals\n200\n40\n80\n80\n400\n\n\n\nFollow the step-by-step guide below:\n\nFirstly, we input the data and create a matrix.\n\n\nR1&lt;-c(20,6,30,44)\nR2&lt;-c(180,34,50,36)\nPersonalityMatrix&lt;-matrix(c(R1,R2),nrow=2,byrow=T,\n      dimnames = list(c(\"Introvert\", \"Extrovert\"),c(\"Red\", \"Yellow\", \"Green\", \"Blue\")))\nPersonalityMatrix\n\n          Red Yellow Green Blue\nIntrovert  20      6    30   44\nExtrovert 180     34    50   36\n\n\n\nWe next visualise the data using side-by-side bar charts.\n\n\nbarplot(PersonalityMatrix,\n        beside=TRUE,\n        ylim=c(0, 200),legend=T, col=c(\"yellow\",\"blue\"),\n        xlab=\"Colour Preference\",\n        ylab=\"Frequency\")\n\n\n\n\n\nOn the face of it, it looks like a higher proportion of Extroverts prefer the colour red, but we will have to now proceed with a statistical approach, i.e. a chi-square test of association using the following code:\n\n\nresult&lt;-chisq.test(PersonalityMatrix)\nresult\n\n\n    Pearson's Chi-squared test\n\ndata:  PersonalityMatrix\nX-squared = 71.2, df = 3, p-value = 2.362e-15\n\n\n\nIn terms of the assumptions, we can check independence by inspection and we check the expected frequencies using the following code:\n\n\nresult$expected\n\n          Red Yellow Green Blue\nIntrovert  50     10    20   20\nExtrovert 150     30    60   60\n\n\n\nThe output shows no expected frequency is less than 5, satisfying the assumption.\nReturning to the chi-square test output, we see that we have a significant result, i.e. we reject the null hypothesis that there is no association and conclude that there appears to be an association between personality and colour preference. Next we calculate the effect size. For this we need the “vcd” package in R, see below:\n\ninstall.packages(\"vcd\")\n\nlibrary(vcd)\nassocstats(PersonalityMatrix)\n\n                    X^2 df   P(&gt; X^2)\nLikelihood Ratio 70.066  3 4.1078e-15\nPearson          71.200  3 2.3315e-15\n\nPhi-Coefficient   : NA \nContingency Coeff.: 0.389 \nCramer's V        : 0.422 \n\n\n\nCramer’s V of 0.422 indicates a medium to strong effect. Finally, we perform post hoc testing using the standardized residuals approach.\n\ninstall.packages(\"questionr\")\n\nlibrary(questionr)\nchisq.residuals(PersonalityMatrix)\n\n            Red Yellow Green  Blue\nIntrovert -4.24  -1.26  2.24  5.37\nExtrovert  2.45   0.73 -1.29 -3.10\n\n\n\nWe can think of the standardized residuals as \\(z\\)-scores, therefore, we look for \\[\n|\\text{standardized  residual}|&gt;1.96\n\\] to be significant at the 5% level. Therefore, the values -3.1 and 2.45 are significant for Extrovert, which indicates that fewer extroverts prefer blue and more extroverts prefer red. The values 5.37 and -4.24 for Introvert are also significant, indicating that more introverts prefer blue, but fewer introverts prefer red.\n\n\n\nExercise 1\nInvestigate, checking the relevant assumptions, whether there is any relationship between Male and Female staff and Earnings based on the data provided by a local company below:\n\n\n\n\nMale\nFemale\n\n\n\n\nHigh Earnings\n37\n39\n\n\nLow Earnings\n25\n23\n\n\n\nIf there is an association investigate this.\nHint: Clearly this is a \\(2\\times2\\) contingency table and hence we must use Yates’ continuity correction (checking assumptions). In R, this is performed using the correct=T option of the chi-square test, see below:\nresult2&lt;-chisq.test(EarningsMatrix, correct=T)\nresult2\n\n\nSolutions\n\nNote that Yates’ continuity correction should be used in this example.\nWe first input the data and create a matrix.\n\n\nRow3&lt;-c(37,39)\nRow4&lt;-c(25,23)\n\nEarningsMatrix&lt;-matrix(c(Row3,Row4), byrow=T,nrow=2, dimnames=list(c(\"High\",\"Low\"),c(\"Male\",\"Female\")))\nEarningsMatrix\n\n     Male Female\nHigh   37     39\nLow    25     23\n\n\n\nIt is good practice to visualise the data.\n\n\nbarplot(EarningsMatrix,\n        beside=TRUE,\n        ylim=c(0, 50),legend=T, col=c(\"yellow\",\"blue\"),\n        xlab=\"Earnings\",\n        ylab=\"Frequency\")\n\n\n\n\n\nNext we perform the Chi-square test of association\n\n\nresult2&lt;-chisq.test(EarningsMatrix, correct=T)\nresult2\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  EarningsMatrix\nX-squared = 0.033991, df = 1, p-value = 0.8537\n\n\n\nWe should also check the expected frequencies assumption\n\n\nresult2$expected\n\n     Male Female\nHigh   38     38\nLow    24     24\n\n\n\nNote that no expected frequency is &lt;5. Independence is also satisfied by visualisation.\nThe p-value is 0.8537&gt;0.05, therefore we conclude that there does not appear to be a relationship between earnings and male/female staff.\n\n\n\nExample 2\nThis example will look at using the linear-by-linear option when testing for association between ordinal variables (see Remark 3.3 in the lecture notes). Independence may be assumed, but check the expected frequencies assumption. We will investigate whether there is an association between education levels and where people live using the following fictitious dataset.\n\n\n\n\nGCSE\nA-level\nDegree\n\n\n\n\nVillage\n130\n50\n20\n\n\nCity\n40\n50\n110\n\n\n\nFollow the steps below in R.\n\nTask: input the variables and create a matrix called EducationMatrix.\nCheck your matrix agains the output below:\n\n\n\n        GCSE A-level Degree\nVillage  130      50     20\nCity      40      50    110\n\n\n\nNext we visualise the data using bar charts:\n\n\nbarplot(EducationMatrix,\n        beside=TRUE,\n        ylim=c(0, 170),legend=T, col=c(\"yellow\",\"blue\"),\n        xlab=\"Education\",\n        ylab=\"Frequency\")\n\n\n\n\n\nNext we perform the Chi-square test of association in order to check the expected frequencies assumption. (Independence is clear by inspection.)\n\n\nresult3&lt;-chisq.test(EducationMatrix)\nresult3\n\n\n    Pearson's Chi-squared test\n\ndata:  EducationMatrix\nX-squared = 109.95, df = 2, p-value &lt; 2.2e-16\n\nresult3$expected\n\n        GCSE A-level Degree\nVillage   85      50     65\nCity      85      50     65\n\n\n\nNote that no expected frequency is \\(&lt;5\\). We now perform the linear-by-linear test (as we have an ordinal variable) using the code below. Note that we first need to covret the data into “table” form.\n\n\ntab&lt;-as.table(EducationMatrix)\ntab\n\n        GCSE A-level Degree\nVillage  130      50     20\nCity      40      50    110\n\nlibrary(coin)\nresult4&lt;-lbl_test(tab)\nresult4\n\n\n    Asymptotic Linear-by-Linear Association Test\n\ndata:  Var2 (ordered) by Var1 (Village, City)\nZ = -10.449, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\nThis result is significant, hence there appears to be an association between location and educational levels. Next we calculate Cramer’s V:\n\n\nlibrary(vcd)\nassocstats(EducationMatrix)\n\n                    X^2 df P(&gt; X^2)\nLikelihood Ratio 118.76  2        0\nPearson          109.95  2        0\n\nPhi-Coefficient   : NA \nContingency Coeff.: 0.464 \nCramer's V        : 0.524 \n\n\n\nAs we have a significant result, we finally perform a post hoc test.\n\n\nlibrary(questionr)\nchisq.residuals(EducationMatrix)\n\n         GCSE A-level Degree\nVillage  4.88       0  -5.58\nCity    -4.88       0   5.58\n\n\n\nThis, together with the bar charts, indicates that more people in cities have a degree as their highest qualification, while more people in the villages have GCSEs as their highest qualification.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "MA-292 lab 7 site.html",
    "href": "MA-292 lab 7 site.html",
    "title": "Lab 7",
    "section": "",
    "text": "Please note that there is a file on Canvas called Getting started with R which may be of some use. This provides details of setting up R and Rstudio on your own computer as well as providing an overview of inputting and importing various data files into R. This should mainly serve as a reminder.\nRecall that we can clear the environment using rm(list=ls()) It is advisable to do this before attempting new questions if confusion may arise with variable names etc.\n\nExample 1\nIn this example we let \\(X\\sim \\text{Po}(0.75)\\) and we calculate \\(P(X=3)\\) and \\(P(X\\leq 2)\\), and we will plot this distribution.\n\nInput the code below to calculate the probabilities. Note the help functions that are useful for determining the arguments.\n\n\nhelp(\"dpois\")\ndpois(3,0.75)\n\n[1] 0.03321327\n\nppois(2,0.75)\n\n[1] 0.9594946\n\n\n\nFor the plot we will create a sequence of 15 numbers, calculate the corresponding probability mass function and the plot these against each other.\n\n\nx&lt;-seq(1,15, length=15)\ndx&lt;-dpois(x,0.75)\nplot(x,dx, xlab=\"x value\",type=\"p\", ylab=\"Probability\", main=\"Poisson(0.75) Distribution\")\n\n\n\n\n\n\nExercise 1\nFor \\(Y\\sim Po(1.5)\\) find \\(P(Y=1)\\) and \\(P(Y&gt;3)\\).\n\n\nExample 2\nIn this example we will create Poisson random numbers and then test to see whether the numbers generated follow a Poisson distribution.\nNote: Everyone’s results will likely be different as this is a random process.\n\nFirstly, let’s create and view a random sample of size 20 from a \\(Po(2.5)\\) distribution.\n\n\nhelp(\"rpois\")\nRandomSample&lt;-rpois(20,2.5)\nRandomSample\n\n [1] 1 2 1 5 0 4 1 2 3 0 1 2 1 0 4 5 2 4 2 1\n\n\n\nNext we consider the mean, variance and a visualisation of the sample:\n\n\nmean(RandomSample)\n\n[1] 2.05\n\nvar(RandomSample)\n\n[1] 2.576316\n\nhist(RandomSample)\n\n\n\n\n\n\n\nExercise 2\nRepeat the example above, but change the mean of the Poisson distribution that you initially generate random numbers from.\n\n\nExample 3\nThis example considers creating a log-linear model for count data, i.e. Poisson regression. In the lectures, estimates of the parameters were just stated; now we will use R to obtain these. We will see the full details of the Surgery example and investigate whether Number of Surgery Visits is related to Location via a log-linear model. We assume a Poisson distributed dependent variable and independence.\n\nWe first import the dataset and check it using the code below:\n\nlibrary(foreign)\nSurgery&lt;-read.spss(file.choose(), to.data.frame = T)\n\nhead(Surgery)\n\n  Patient Location SurgeryVisits\n1       1        0             1\n2       2        0             2\n3       3        1             5\n4       4        1             6\n5       5        1             5\n6       6        0             1\n\n\n\nNext we perform the log-linear/Poisson regression analysis\n\n\nSurgeryPoissonReg&lt;-glm(SurgeryVisits~factor(Location), data=Surgery, family=\"poisson\")\nsummary(SurgeryPoissonReg)\n\n\nCall:\nglm(formula = SurgeryVisits ~ factor(Location), family = \"poisson\", \n    data = Surgery)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.4138  -0.3379  -0.2238   0.1303   2.0000  \n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)         0.8473     0.2673   3.170  0.00152 **\nfactor(Location)1   0.7033     0.3190   2.205  0.02745 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 14.1586  on 12  degrees of freedom\nResidual deviance:  8.9034  on 11  degrees of freedom\nAIC: 52.038\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nYou should obtain the following output:\n\n\n\nTo help us evaluate the model we can use the following code which provides a \\(p-\\)value for the Deviance statistic and Wald chi-square statistics:\n\n\nanova(SurgeryPoissonReg, test=\"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: SurgeryVisits\n\nTerms added sequentially (first to last)\n\n                 Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)  \nNULL                                12    14.1586           \nfactor(Location)  1   5.2551        11     8.9034  0.02188 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe Residual deviance/df\\(=\\tfrac{8.9034}{11}=0.81\\) is close to 1, (and the corresponding p-value is \\(0.02188&lt;0.05\\)) indicating a good model fit.\n\ninstall.packages(\"lmtest\")\n\nlibrary(lmtest)\nhelp(\"waldtest\")\nwaldtest(SurgeryPoissonReg, test=\"Chisq\")\n\nWald test\n\nModel 1: SurgeryVisits ~ factor(Location)\nModel 2: SurgeryVisits ~ 1\n  Res.Df Df  Chisq Pr(&gt;Chisq)  \n1     11                       \n2     12 -1 4.8621    0.02745 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe Wald Chi-Square statistic is significant for Location (p-value of \\(0.027&lt;0.05\\)) indicating the significance of the Location variable.\nFurther, from the earlier output, note that \\(\\hat{b}_0=0.847\\); and \\(\\hat{b}_1=0.703\\), i.e. we have the model, \\[\n\\ln \\hat{\\lambda}_i=0.847+0.703x_{i1}, \\text{ or }\n\\hat{\\lambda}_i=e^{0.847+0.703x_{i1}}=e^{0.847}(e^{0.703})^{x_{i1}}.\n\\]\n\n\n\nExercise 3\nThe Poisson Log-linear (PCP Defaults) dataset contains the number of defaults on personal contract plans (PCPs) for a certain car dealership according to their customers’ salary band. The Salary variable has 3 categories, 0, 1 and 2. Create a Poisson log-linear model for these data. You may assume independence and that the dependent variable is Poisson distributed. Make sure you analyse the output.\nHint: In this question the categorical variable (Salary) has more than 2 categories - this makes interpreting the model more complicated. The notes on dummy variables in the lecture notes should help you, along with Example 3 on the R examples class sheet.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "MA-292 lab 6 site - with solutions.html",
    "href": "MA-292 lab 6 site - with solutions.html",
    "title": "Lab 6 - with solutions",
    "section": "",
    "text": "Please note that there is a file on Canvas called Getting started with R which may be of some use. This provides details of setting up R and Rstudio on your own computer as well as providing an overview of inputting and importing various data files into R. This should mainly serve as a reminder.\nRecall that we can clear the environment using rm(list=ls()) It is advisable to do this before attempting new questions if confusion may arise with variable names etc.\n\nExample 1\nIn this example we will see how to calculate probabilities from the hypergeometric distribution.\n\nWe will calculate the probability as seen in Example 3.8 in the lecture notes. Recall Example 3.8:\n\n“Suppose we have a box of 20 marbles in which 8 are blue and 12 are red. Ten marbles are taken from the box without replacement. Find the probability of obtaining exactly 3 blue marbles.”\n\nIn this case, let \\(X\\) denote the number of blue marbles, i.e. \\(X\\sim\\text{hypergeometric}(20, 8, x)\\) and therefore we want to find \\(P(X=3)\\). We further will calculate \\(P(X\\leq 2)\\) and \\(P(X\\geq 6)\\). Use the help function to understand the parameters.\n\n\nhelp(\"dhyper\")\ndhyper(3,8,12,10)\n\n[1] 0.2400572\n\nphyper(2,8,12,10)\n\n[1] 0.08490117\n\nphyper(5,8,12,10,lower.tail = F)\n\n[1] 0.08490117\n\n\n\n\nExercise 1\nIf we add 5 blue marbles to the box in the example above, but keep the sample the same (10), what is \\(P(X&gt;4)\\)?\n\n\nSolution\n\nphyper(4,13,12,10,lower.tail = F)\n\n[1] 0.7158837\n\n\n\n\nExercise 2\nSuppose we now have a situation where we have 10 green and 20 white marbles and 15 are selected. Let \\(Y\\) denote the number of green marbles selected, i.e. \\[Y\\sim\\text{hypergeometric}(30,10,y).\\] Calculate \\(P(Y=5)\\) and \\(P(Y&lt;3)\\).\n\n\nSolution\n\n\\(P(Y=5):\\)\n\n\ndhyper(5,10,20,15)\n\n[1] 0.3001499\n\n\n\n\\(P(Y&lt;3):\\)\n\n\nphyper(2,10,20,15)\n\n[1] 0.02508746\n\n\n\n\nExample 2\nIn this example we will see how to use Fisher’s Exact Test in R. Firstly, remind yourself why we would use this test over the \\(\\chi^2\\) test - see the lecture notes or full details. This is important when interpreting the R output.\nWe will consider Example 3.9 in the lecture notes where we investigate whether there is a relationship between attendance and exam results. Clearly we have independence, i.e. each member can only contribute to one entry in the table:\n\n\n\n\nPass\nFail\nTotals\n\n\n\n\nAttendance \\(\\mathbf{&gt;50\\%}\\)\n10\n2\n12\n\n\nAttendance \\(\\mathbf{\\leq50\\%}\\)\n5\n7\n12\n\n\nTotals\n15\n9\n24\n\n\n\n\nRecall the hypotheses:\n\n\\(H_0\\): there is no association between attendance and passing the exam;\n\\(H_1\\): if attendance \\(&gt;50\\%\\) students are more likely to pass (one-tailed).\n\nFirstly input the data into R and create a matrix:\n\n\nRow1&lt;-c(10,2)\nRow2&lt;-c(5,7)\n\nAttendanceMatrix&lt;-matrix(c(Row1,Row2), byrow=T,nrow=2,\n          dimnames=list(c(\"&gt;50\",\"50 or less\"),c(\"Pass\",\"Fail\")))\nAttendanceMatrix\n\n           Pass Fail\n&gt;50          10    2\n50 or less    5    7\n\n\n\nNext we visualise the data to get a feeling for potential relationships.\n\n\nbarplot(AttendanceMatrix,\n        beside=TRUE,\n        ylim=c(0, 30),legend=T, col=c(\"yellow\",\"blue\"),\n        xlab=\"Pass/Fail\",\n        ylab=\"Frequency\")\n\n\n\n\n\nNext we perform the Chi-square test of association, but we will see that the expected frequency assumption fails:\n\n\nresult&lt;-chisq.test(AttendanceMatrix, correct=T)\nresult\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  AttendanceMatrix\nX-squared = 2.8444, df = 1, p-value = 0.09169\n\nresult$expected\n\n           Pass Fail\n&gt;50         7.5  4.5\n50 or less  7.5  4.5\n\n\n\nAs the expected frequency assumption fails (we have independence by inspection) we now proceed with Fisher’s exact test. The help function should be used to make sure we perform an appropriate one-tailed test.\n\n\nhelp(\"fisher.test\")\nfisher.test(AttendanceMatrix, alternative = \"greater\")\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  AttendanceMatrix\np-value = 0.04469\nalternative hypothesis: true odds ratio is greater than 1\n95 percent confidence interval:\n 1.044487      Inf\nsample estimates:\nodds ratio \n  6.394505 \n\n\n\nLooking at the statistic for this test in the output tables, we see that we have a p-value of \\(0.045&lt;0.05\\), therefore we reject \\(H_0\\) and conclude that evidence suggests that if students attend more than 50% of lectures then they are more likely to pass the exam. This result confirms our manual calculations in the lecture.\n\n\n\nExercise 3\nThe table below contains information on the number of males and females earning low, medium and high salaries in a particular institution.\n\n\n\n\nLow\nMedium\nHigh\nTotals\n\n\n\n\nMale\n6\n58\n3\n67\n\n\nFemale\n3\n44\n10\n57\n\n\nTotals\n9\n102\n13\n\n\n\n\nInvestigate whether there is an association between salary level and male and female earnings, justifying the method you use.\n\n\nSolution\n\nWe first input the data and create a matrix\n\n\nRow3&lt;-c(6,58,3)\nRow4&lt;-c(3,44,10)\n\nSalariesMatrix&lt;-matrix(c(Row3,Row4), byrow=T,nrow=2, dimnames=list(c(\"Male\",\"Female\"),c(\"Low\",\"Medium\", \"High\")))\nSalariesMatrix\n\n       Low Medium High\nMale     6     58    3\nFemale   3     44   10\n\n\n\nIt is good practice to visualise the data.\n\n\nbarplot(SalariesMatrix,\n        beside=TRUE,\n        ylim=c(0, 70),legend=T, col=c(\"yellow\",\"blue\"),\n        xlab=\"Salaries\",\n        ylab=\"Frequency\")\n\n\n\n\n\nNext we perform the Chi-square test of association.\n\n\nresult2&lt;-chisq.test(SalariesMatrix)\nresult2\n\n\n    Pearson's Chi-squared test\n\ndata:  SalariesMatrix\nX-squared = 5.9229, df = 2, p-value = 0.05174\n\n\n\nHowever, when we check the expected frequencies assumption we see it fails.\n\n\nresult2$expected\n\n            Low  Medium     High\nMale   4.862903 55.1129 7.024194\nFemale 4.137097 46.8871 5.975806\n\n\n\nHence we use Fisher’s exact test as more than 20% of expected counts are &lt;5 (independence is still required).\nThe help function should be used to make sure we perform an appropriate two-tailed test.\n\nhelp(\"fisher.test\")\n\nfisher.test(SalariesMatrix)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  SalariesMatrix\np-value = 0.04497\nalternative hypothesis: two.sided\n\n\n\nThis is a significant result, hence there seems to be an association between salary and male and female employees.\nVisual inspection of the barplots indicates that there are more female higher earners, but more male middle earners.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "MA-292 lab 8 site.html",
    "href": "MA-292 lab 8 site.html",
    "title": "Lab 8",
    "section": "",
    "text": "Please note that there is a file on Canvas called Getting started with R which may be of some use. This provides details of setting up R and Rstudio on your own computer as well as providing an overview of inputting and importing various data files into R. This should mainly serve as a reminder.\nRecall that we can clear the environment using rm(list=ls()) It is advisable to do this before attempting new questions if confusion may arise with variable names etc.\n\nExercise 1\nLet \\(X\\sim Bin(10,0.4)\\). Calculate the following. The help(\"pbinom\") function may be of use.\n\nFind \\(P(X=2)\\).\nNow we use the distribution function pbinom to find \\(P(X&lt;5)\\).\nGenerate 30 random values from \\(Bin(20,0.1)\\).\nPlot the Probability Mass Function (PMF) of \\(Bin(15,0.3)\\). Hint: create a sequence of numbers (inputs) between 1 and 15 and then find the corresponding values of the distribution function (PMF in this case). Finally, we plot these against each other.\n\n\n\nExample 1\nIn this example we will use the logistic regression procedure on the Logistic Regression (Quality) dataset. This dataset has a number of variables related to the quality of care. We firstly assume that we have independence of observations. Further remarks about assumptions will be made in the steps below. Please follow the steps below.\n\nThe first model we will consider is where PoorCare is the dependent variable and AcuteDrugGapSmall is the independent variable. Note that PoorCare is a binary variable, satisfying an assumption of the procedure. Inspection of AcuteDrugGapSmall indicates the variable can be treated as continuous; therefore a further assumption is satisfied. We do not need to consider multicollinearity in this case as we only have one independent variable. The final assumption is linearity in the sense that Logit(PoorCare) and AcuteDrugGapSmall should be linearly related - we assume that this is true in this case.\nWe firstly import and check the data:\n\nlibrary(foreign)\nQuality&lt;-read.spss(file.choose(), to.data.frame = T)\n\nhead(Quality)\n\n  MemberID InpatientDays ERVisits OfficeVisits Narcotics DaysSinceLastERVisit\n1        1             0        0           18         1                  731\n2        2             1        1            6         1                  411\n3        3             0        0            5         3                  731\n4        4             0        1           19         0                  158\n5        5             8        2           19         3                  449\n6        6             2        0            9         2                  731\n  Pain TotalVisits ProviderCount MedicalClaims ClaimLines StartedOnCombination\n1   10          18            21            93        222                    0\n2    0           8            27            19        115                    0\n3   10           5            16            27        148                    0\n4   34          20            14            59        242                    0\n5   10          29            24            51        204                    0\n6    6          11            40            53        156                    0\n  AcuteDrugGapSmall PoorCare\n1                 0        0\n2                 1        0\n3                 5        0\n4                 0        0\n5                 0        0\n6                 4        1\n\n\n\nWe now fit the model.\n\n\nQualityLogit&lt;-glm(PoorCare~AcuteDrugGapSmall, data=Quality, family=\"binomial\")\nsummary(QualityLogit)\n\n\nCall:\nglm(formula = PoorCare ~ AcuteDrugGapSmall, family = \"binomial\", \n    data = Quality)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.8704  -0.6252  -0.5555  -0.2776   1.9724  \n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -1.79086    0.28220  -6.346 2.21e-10 ***\nAcuteDrugGapSmall  0.25762    0.06451   3.994 6.50e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 147.88  on 130  degrees of freedom\nResidual deviance: 124.54  on 129  degrees of freedom\nAIC: 128.54\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nWe begin to evaluate it using Deviance and Wald chi-square statistics:\n\n\nanova(QualityLogit, test=\"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: PoorCare\n\nTerms added sequentially (first to last)\n\n                  Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)    \nNULL                                130     147.88             \nAcuteDrugGapSmall  1   23.336       129     124.54 1.36e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\ninstall.packages(\"lmtest\")\n\nlibrary(lmtest)\nwaldtest(QualityLogit, test=\"Chisq\")\n\nWald test\n\nModel 1: PoorCare ~ AcuteDrugGapSmall\nModel 2: PoorCare ~ 1\n  Res.Df Df Chisq Pr(&gt;Chisq)    \n1    129                        \n2    130 -1 15.95  6.504e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nFrom this table we see that the Wald test shows that the coefficient of AcuteDrugSmallGap, i.e. \\(\\beta_1\\) is significantly different to 0. The Deviance statistic is also significant. Using the theory in the lecture notes we conclude that the model is: \\[\nlogit(\\hat{p})=-1.791+0.258x_1 \\quad \\text{or}\\quad \\hat{p}=\\frac{e^{-1.791+0.258x_1}}{1+e^{-1.791+0.258x_1}},\n\\] where \\(x_1\\) denotes AcuteDrugGapSmall and \\(\\hat{p}\\) is the estimated probability of receiving poor care. Note that the Fisher scoring iterations indicate how quickly the MLEs for the coefficients in the model converge.\nThe following code provides \\(e^\\beta\\) along with a confidence interval for the coefficient, which helps our analysis\n\n\nexp(cbind(OR = coef(QualityLogit), confint(QualityLogit)))\n\n                         OR      2.5 %    97.5 %\n(Intercept)       0.1668162 0.09262101 0.2818085\nAcuteDrugGapSmall 1.2938500 1.15026698 1.4848021\n\n\n\nNote that \\(e^\\beta=1.294\\), indicating that increasing the independent variable, AcuteDrugsSmallGap, by 1 unit will increase the odds of PoorCare by 29.4%. The 95% confidence interval for \\(e^\\beta\\) is \\((1.15,1.484)\\) - it is good that the confidence interval does not include 1, otherwise this would represent no change to the odds of the outcome.\nWe next use the DescTools package to calculate some pseudo\\(R^2\\) statistics.\n\ninstall.packages(\"DescTools\")\n\nlibrary(DescTools)\nPseudoR2(QualityLogit, which=c(\"CoxSnell\", \"Nagelkerke\"))\n\n  CoxSnell Nagelkerke \n 0.1631750  0.2411715 \n\n\n\nWe see that Nagelkerke’s \\(R^2\\) indicates that 24.1% of the variation in the outcome is accounted for by the model (Cox and Snell: 16.3%)\nFinally, we further evaluate the model using the Hosmer and Lemeshow test:\n\ninstall.packages(\"generalhoslem\")\n\nlibrary(generalhoslem) \nlogitgof(Quality$PoorCare, fitted(QualityLogit))\n\n\n    Hosmer and Lemeshow test (binary model)\n\ndata:  Quality$PoorCare, fitted(QualityLogit)\nX-squared = 1.9916, df = 3, p-value = 0.5741\n\n\n\nThe Hosmer and Lemeshow test shows that we have a good model fit (not significant).\nWe now include another independent variable, StartedOnCombination. As the new variable is categorical, we must inform R using the following code:\n\n\nCombination&lt;-factor(Quality$StartedOnCombination)\n\n\nWe now proceed with the model:\n\n\nQualityLogit2&lt;-glm(PoorCare~AcuteDrugGapSmall+Combination, data=Quality, family=\"binomial\")\nsummary(QualityLogit2)\n\n\nCall:\nglm(formula = PoorCare ~ AcuteDrugGapSmall + Combination, family = \"binomial\", \n    data = Quality)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.9085  -0.5564  -0.4868  -0.2433   2.0937  \n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -2.07335    0.31625  -6.556 5.52e-11 ***\nAcuteDrugGapSmall  0.28600    0.06911   4.138 3.50e-05 ***\nCombination1       3.43061    1.15381   2.973  0.00295 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 147.88  on 130  degrees of freedom\nResidual deviance: 112.25  on 128  degrees of freedom\nAIC: 118.25\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nNote that the Fisher scoring iterations indicate how quickly the MLEs for the coefficients in the model converge.\nAgain we begin to evaluate the model using:\n\n\nanova(QualityLogit2, test=\"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: PoorCare\n\nTerms added sequentially (first to last)\n\n                  Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \nNULL                                130     147.88              \nAcuteDrugGapSmall  1   23.336       129     124.54  1.36e-06 ***\nCombination        1   12.293       128     112.25 0.0004545 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlibrary(lmtest)\nwaldtest(QualityLogit2, test=\"Chisq\")\n\nWald test\n\nModel 1: PoorCare ~ AcuteDrugGapSmall + Combination\nModel 2: PoorCare ~ 1\n  Res.Df Df  Chisq Pr(&gt;Chisq)    \n1    128                         \n2    130 -2 22.791  1.125e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nWe see that the Wald test shows that the coefficients of both independent variables i.e. \\(\\beta_1\\) and \\(\\beta_2\\) are significantly different to 0. The Deviance statistic is also significant.\nWe conclude that including the variable AcuteDrugGapSmall in logistic regression significantly improves the model and also that including both AcuteDrugGapSmall and StartedOnCombination significantly improves the model even further. Therefore, the model in this case is: \\[\nlogit(\\hat{p})=-2.073+0.286x_1+3.431x_2  \\quad \\text{or}\\quad \\hat{p}=\\frac{e^{-2.073+0.286x_1+3.431x_2  }}{1+e^{-2.073+0.286x_1+3.431x_2 }},\n\\] where \\(x_1\\) denotes AcuteDrugGapSmall, \\(x_2\\) denotes StartedOnCombination and \\(\\hat{p}\\) is the estimated probability of receiving poor care.\nAs we will now have two independent variables we must check for no multicollinearity using the following R code:\n\ninstall.packages(\"car\")\n\nlibrary(car)\nvif(QualityLogit2)\n\nAcuteDrugGapSmall       Combination \n         1.022965          1.022965 \n\n\n\nWe see from the above table that both the VIF terms are around 1, satisfying the no-multicollinearity assumption - we require the VIF factors to be around 1, or at least \\(0.01&lt;VIF&lt;5\\).\nThe following code provides \\(e^\\beta\\) along with a confidence interval for the coefficient, which helps our analysis\n\n\nexp(cbind(OR = coef(QualityLogit2), confint(QualityLogit2)))\n\n                          OR      2.5 %      97.5 %\n(Intercept)        0.1257643 0.06450324   0.2248477\nAcuteDrugGapSmall  1.3310961 1.17478068   1.5444554\nCombination1      30.8956101 4.27766048 629.2075641\n\n\n\nNeither confidence interval our predictor variables for \\(e^\\beta\\) includes 1.\nWe next use the DescTools package to calculate some pseudo\\(R^2\\) statistics.\n\n\nPseudoR2(QualityLogit2, which=c(\"CoxSnell\", \"Nagelkerke\"))\n\n  CoxSnell Nagelkerke \n 0.2381334  0.3519595 \n\n\n\nWe see that Nagelkerke’s \\(R^2\\) indicates that 35.2% of the variation in the outcome is accounted for by the model (Cox and Snell: 23.8%)\nFinally, we further evaluate the model using the Hosmer and Lemeshow test:\n\n\nlogitgof(Quality$PoorCare, fitted(QualityLogit2))\n\n\n    Hosmer and Lemeshow test (binary model)\n\ndata:  Quality$PoorCare, fitted(QualityLogit2)\nX-squared = 0.70579, df = 4, p-value = 0.9506\n\n\n\nThe Hosmer and Lemeshow test shows that we have a good model fit (not significant).\n\n\n\nExercise 2\nInvestigate the Penalty dataset to determine whether a logistic regression model may be used to predict the variable Scored from the three variables PSWQ, Anxious and Previous.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "MA-292 lab 7 site - with solutions.html",
    "href": "MA-292 lab 7 site - with solutions.html",
    "title": "Lab 7 - with solutions",
    "section": "",
    "text": "Please note that there is a file on Canvas called Getting started with R which may be of some use. This provides details of setting up R and Rstudio on your own computer as well as providing an overview of inputting and importing various data files into R. This should mainly serve as a reminder.\nRecall that we can clear the environment using rm(list=ls()) It is advisable to do this before attempting new questions if confusion may arise with variable names etc.\n\nExample 1\nIn this example we let \\(X\\sim \\text{Po}(0.75)\\) and we calculate \\(P(X=3)\\) and \\(P(X\\leq 2)\\), and we will plot this distribution.\n\nInput the code below to calculate the probabilities. Note the help functions that are useful for determining the arguments.\n\n\nhelp(\"dpois\")\ndpois(3,0.75)\n\n[1] 0.03321327\n\nppois(2,0.75)\n\n[1] 0.9594946\n\n\n\nFor the plot we will create a sequence of 15 numbers, calculate the corresponding probability mass function and the plot these against each other.\n\n\nx&lt;-seq(1,15, length=15)\ndx&lt;-dpois(x,0.75)\nplot(x,dx, xlab=\"x value\",type=\"p\", ylab=\"Probability\", main=\"Poisson(0.75) Distribution\")\n\n\n\n\n\n\nExercise 1\nFor \\(Y\\sim Po(1.5)\\) find \\(P(Y=1)\\) and \\(P(Y&gt;3)\\).\n\n\nSolution\n\n\\(P(Y=1):\\)\n\n\ndpois(1,1.5)\n\n[1] 0.3346952\n\n\n\n\\(P(Y&gt;3):\\)\n\n\nppois(3,1.5, lower.tail=F)\n\n[1] 0.06564245\n\n\n\n\nExample 2\nIn this example we will create Poisson random numbers and then test to see whether the numbers generated follow a Poisson distribution.\nNote: Everyone’s results will likely be different as this is a random process.\n\nFirstly, let’s create and view a random sample of size 20 from a \\(Po(2.5)\\) distribution.\n\n\nhelp(\"rpois\")\nRandomSample&lt;-rpois(20,2.5)\nRandomSample\n\n [1] 4 0 2 4 2 0 5 2 5 0 5 3 2 0 4 2 5 2 4 4\n\n\n\nNext we consider the mean, variance and a visualisation of the sample:\n\n\nmean(RandomSample)\n\n[1] 2.75\n\nvar(RandomSample)\n\n[1] 3.25\n\nhist(RandomSample)\n\n\n\n\n\n\n\nExercise 2\nRepeat the example above, but change the mean of the Poisson distribution that you initially generate random numbers from.\n\n\nSolution\nThis will depend on your individual choice of the mean of the distribution (and the randomness of the procedure).\n\n\nExample 3\nThis example considers creating a log-linear model for count data, i.e. Poisson regression. In the lectures, estimates of the parameters were just stated; now we will use R to obtain these. We will see the full details of the Surgery example and investigate whether Number of Surgery Visits is related to Location via a log-linear model. We assume a Poisson distributed dependent variable and independence.\n\nWe first import the dataset and check it using the code below:\n\nlibrary(foreign)\nSurgery&lt;-read.spss(file.choose(), to.data.frame = T)\n\nhead(Surgery)\n\n  Patient Location SurgeryVisits\n1       1        0             1\n2       2        0             2\n3       3        1             5\n4       4        1             6\n5       5        1             5\n6       6        0             1\n\n\n\nNext we perform the log-linear/Poisson regression analysis\n\n\nSurgeryPoissonReg&lt;-glm(SurgeryVisits~factor(Location), data=Surgery, family=\"poisson\")\nsummary(SurgeryPoissonReg)\n\n\nCall:\nglm(formula = SurgeryVisits ~ factor(Location), family = \"poisson\", \n    data = Surgery)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.4138  -0.3379  -0.2238   0.1303   2.0000  \n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)         0.8473     0.2673   3.170  0.00152 **\nfactor(Location)1   0.7033     0.3190   2.205  0.02745 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 14.1586  on 12  degrees of freedom\nResidual deviance:  8.9034  on 11  degrees of freedom\nAIC: 52.038\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nYou should obtain the following output:\n\n\n\nTo help us evaluate the model we can use the following code which provides a \\(p-\\)value for the Deviance statistic and Wald chi-square statistics:\n\n\nanova(SurgeryPoissonReg, test=\"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: SurgeryVisits\n\nTerms added sequentially (first to last)\n\n                 Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)  \nNULL                                12    14.1586           \nfactor(Location)  1   5.2551        11     8.9034  0.02188 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe Residual deviance/df\\(=\\tfrac{8.9034}{11}=0.81\\) is close to 1, (and the corresponding p-value is \\(0.02188&lt;0.05\\)) indicating a good model fit.\n\ninstall.packages(\"lmtest\")\n\nlibrary(lmtest)\nhelp(\"waldtest\")\nwaldtest(SurgeryPoissonReg, test=\"Chisq\")\n\nWald test\n\nModel 1: SurgeryVisits ~ factor(Location)\nModel 2: SurgeryVisits ~ 1\n  Res.Df Df  Chisq Pr(&gt;Chisq)  \n1     11                       \n2     12 -1 4.8621    0.02745 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe Wald Chi-Square statistic is significant for Location (p-value of \\(0.027&lt;0.05\\)) indicating the significance of the Location variable.\nFurther, from the earlier output, note that \\(\\hat{b}_0=0.847\\); and \\(\\hat{b}_1=0.703\\), i.e. we have the model, \\[\n\\ln \\hat{\\lambda}_i=0.847+0.703x_{i1}, \\text{ or }\n\\hat{\\lambda}_i=e^{0.847+0.703x_{i1}}=e^{0.847}(e^{0.703})^{x_{i1}}.\n\\]\n\n\n\nExercise 3\nThe Poisson Log-linear (PCP Defaults) dataset contains the number of defaults on personal contract plans (PCPs) for a certain car dealership according to their customers’ salary band. The Salary variable has 3 categories, 0, 1 and 2. Create a Poisson log-linear model for these data. You may assume independence and that the dependent variable is Poisson distributed. Make sure you analyse the output.\nHint: In this question the categorical variable (Salary) has more than 2 categories - this makes interpreting the model more complicated. The notes on dummy variables in the lecture notes should help you, along with Example 3 on the R examples class sheet.\n\n\nSolution\n\nWe first import and check the dataset.\n\nlibrary(foreign)\nPCP&lt;-read.spss(file.choose(), to.data.frame = T)\n\nhead(PCP)\n\n  Customer Salary DefaultNumber\n1        1      0             5\n2        2      0             6\n3        3      1             3\n4        4      2             3\n5        5      2             5\n6        6      1             4\n\nattach(PCP)\n\n\nWe now perfom the loglinear/Poisson regression analysis.\n\n\nPCPReg&lt;-glm(DefaultNumber~factor(Salary), data=PCP, family=\"poisson\")\nsummary(PCPReg)\n\n\nCall:\nglm(formula = DefaultNumber ~ factor(Salary), family = \"poisson\", \n    data = PCP)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1213  -0.2583   0.0000   0.1671   1.5764  \n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       1.7228     0.1890   9.116   &lt;2e-16 ***\nfactor(Salary)1  -0.6242     0.3200  -1.951   0.0511 .  \nfactor(Salary)2  -0.9118     0.3832  -2.380   0.0173 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 16.5487  on 13  degrees of freedom\nResidual deviance:  8.9861  on 11  degrees of freedom\nAIC: 56.247\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nIn order to evaluate the model (Deviance and Wald chi-square statistics) we use the following code:\n\n\nanova(PCPReg, test=\"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: DefaultNumber\n\nTerms added sequentially (first to last)\n\n               Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)  \nNULL                              13    16.5487           \nfactor(Salary)  2   7.5627        11     8.9861  0.02279 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\ninstall.packages(\"lmtest\")\n\nlibrary(lmtest)\nwaldtest(PCPReg, test=\"Chisq\")\n\nWald test\n\nModel 1: DefaultNumber ~ factor(Salary)\nModel 2: DefaultNumber ~ 1\n  Res.Df Df  Chisq Pr(&gt;Chisq)  \n1     11                       \n2     13 -2 7.3907    0.02484 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnalysis of the output:\n\nThe deviance/df is 0.817, indicating a good model fit. Furthermore, the Wald chi-square statistic is significant indicating the significance of the Salary variable.\nThe three categorical variables are combined into 2 dummy variables, say \\(d_1\\) and \\(d_2\\), where \\(d_1\\) and \\(d_2\\) take the values \\(0\\) or \\(1\\), i.e. we have a model of the form: \\[\ny_i=e^{b_0+c_{1}d_{1}+c_{2}d_{2}}.\n\\]\nIn particular, we have: \\[\ny_i=e^{1.723-0.912d_{1}-0.624d_{2}}.\n\\]\nThe base case is where \\(d_1=d_2=0\\), and this corresponds to salary band \\(0\\) in this case. Therefore, for salary band 0, we have, \\[\ny_i=e^{1.723}=5.6, \\quad \\text{i.e. 5.6 defaults for people in band $0$.}\n\\]\nAnother possibility is when \\(d_1=1\\) and \\(d_2=0\\), and this corresponds to salary band \\(2\\). In this case the model is, \\[\ny_i=e^{1.723-0.912\\times1-0.624\\times 0}=2.25, \\quad \\text{i.e. 2.25 defaults for people in band $2$.}\n\\]\nFinally, for \\(d_1=0\\) and \\(d_2=1\\), i.e. for salary band \\(1\\), we have, \\[\ny_i=e^{1.723-0.912\\times0-0.624\\times 1}=3, \\quad \\text{i.e. 3 defaults for people in band $1$.}\n\\]\n\n\n\n\n\n Back to top"
  }
]